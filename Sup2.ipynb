{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/youseefmoemen/Neural-Network/blob/main/Sup2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GON6iRSbG-PX"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tv84SdZlPrxd"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "  return 1 / (1 + np.exp(-1 * z))\n",
        "def sigmoid_grad(z):\n",
        "  t = sigmoid(z)\n",
        "  return t * (1 - t)\n",
        "def leakyRelu(z):\n",
        "  return np.where(z >= 0, z, z * 0.3)\n",
        "def leakyRelu_grad(z):\n",
        "  return np.where(z >= 0, 1, 0.3)\n",
        "def tanh(z):\n",
        "  return np.tanh(z)\n",
        "def tanh_grad(z):\n",
        "  return 1 - np.power(tanh(z), 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "A8eh2OJgP7IB"
      },
      "outputs": [],
      "source": [
        "Activations = {\n",
        "    'sigmoid': sigmoid,\n",
        "    'leakyRelu': leakyRelu,\n",
        "    'softmax': softmax,\n",
        "    'tanh': tanh\n",
        "}\n",
        "Activations_grad = {\n",
        "    'sigmoid': sigmoid_grad,\n",
        "    'leakyRelu': leakyRelu_grad,\n",
        "    'softmax': softmax_grad,\n",
        "    'tanh': tanh_grad\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "15j6dXKLIWN_"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1Vx8P1VUHdk0"
      },
      "outputs": [],
      "source": [
        "mnist = fetch_openml('mnist_784', version = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "KVLMhO_MIYzq"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "mXY0wY3DIbFX"
      },
      "outputs": [],
      "source": [
        "sss = StratifiedShuffleSplit(n_splits = 1, train_size=50000, test_size=10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "D1VbSFuaIcp5"
      },
      "outputs": [],
      "source": [
        "for train_index, test_index in sss.split(mnist.data, mnist.target):\n",
        "  X_train = mnist.data.iloc[train_index] \n",
        "  y_train = mnist.target[train_index]\n",
        "  X_test = mnist.data.iloc[test_index] \n",
        "  y_test = mnist.target[test_index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "CKFS19L0Ierk"
      },
      "outputs": [],
      "source": [
        "def clip_image(img, clipping_size = 0):\n",
        "  img = img.reshape((28, 28))\n",
        "  img = np.delete(img, range(clipping_size), 0)\n",
        "  img = np.delete(img, range(clipping_size), 1)\n",
        "  return img.flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "75II8JDuIhF3"
      },
      "outputs": [],
      "source": [
        "def momentum(img, step_size, n_steps, clipping_size):\n",
        "  img = img.reshape((28 - clipping_size, 28-clipping_size))\n",
        "  momentums = []\n",
        "  for i in range(n_steps):\n",
        "    x = np.array(list(range(i*step_size, (i+1)*step_size)))\n",
        "    for j in range(n_steps):\n",
        "      pixels = img[j*step_size: (j+1)*step_size, i*step_size: (i+1) * step_size]\n",
        "      y =  np.array(list(range(j*step_size, (j+1)*step_size))).reshape(step_size,1)\n",
        "      area = np.sum(np.sum(pixels))\n",
        "      x_c = np.sum(np.sum(x.T * pixels)) / (area + 1e-10)\n",
        "      y_c = np.sum(np.sum(y * pixels)) / (area + 1e-10)\n",
        "      momentums.append((x_c, y_c))\n",
        "  return momentums"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "_hZcCsCXIimx"
      },
      "outputs": [],
      "source": [
        "def reduction(data = None, n_momentums = 9):\n",
        "  n_steps = math.floor(np.sqrt(n_momentums))\n",
        "  clipping_size = 28 % n_steps\n",
        "  step_size = (28 - clipping_size) // n_steps\n",
        "  data = np.apply_along_axis(clipping_size = clipping_size\n",
        "                      , func1d = clip_image, axis = 1, arr = data)\n",
        "  momentums = np.apply_along_axis(step_size = step_size, n_steps = n_steps,\n",
        "                                  clipping_size= clipping_size,\n",
        "                                  func1d = momentum, axis = 1, arr = data)\n",
        "  return data, momentums"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "XeGU_ZnMIkCS"
      },
      "outputs": [],
      "source": [
        "X_train = reduction(np.array(X_train.copy()), n_momentums = 9)[1].reshape(50000, 18)\n",
        "y_train2 = np.zeros(shape=(y_train.shape[0], 10))\n",
        "for index, instance in enumerate(y_train):\n",
        "  y_train2[index][int(instance)] = 1\n",
        "y_train = y_train2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "JDuzFFNDBMWj"
      },
      "outputs": [],
      "source": [
        "class Layer():\n",
        "  def __init__(self, n_neurons, activation):\n",
        "    self.n_neurons = n_neurons\n",
        "    self.w = None\n",
        "    self.activation = Activations[activation]\n",
        "    self.activation_grad = Activations_grad[activation]\n",
        "  \n",
        "  def initialize(self, prev):\n",
        "    self.w = np.random.normal(size=(self.n_neurons, prev))\n",
        "\n",
        "  def compute(self, Oi):\n",
        "    Netj = np.dot(Oi, self.w.T)  \n",
        "    Oj = self.activation(Netj)\n",
        "    return Netj, Oj\n",
        "  \n",
        "  def grad(self, delta, w_next, Oi):\n",
        "    Netj = self.compute(Oi)[0]\n",
        "    partial = np.dot(delta, w_next)\n",
        "    delta = np.multiply(partial, self.activation_grad(Netj))\n",
        "    Dw = np.dot(delta.T, Oi)\n",
        "    return Dw, delta\n",
        "\n",
        "  def update(self, learning_rate, Dw):\n",
        "    hold = self.w.shape\n",
        "    self.w  = self.w  -  learning_rate * Dw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "OMW5bsyJUuDX"
      },
      "outputs": [],
      "source": [
        "class OutputLayer(Layer):\n",
        "  def __init__(self, *args, **kwargs):\n",
        "    super().__init__(*args, **kwargs)\n",
        "  \n",
        "  def grad(self, target, Oi):\n",
        "    Netj, Oj = self.compute(Oi)\n",
        "    delta = np.multiply(-1* (1/target.shape[0]) * (target - Oj), self.activation_grad(Netj))\n",
        "    Dw = np.dot(delta.T, Oi)\n",
        "    return Dw, delta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "-v5Lkksr4xb5"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork():\n",
        "  def __init__(self, layers, input_shape):\n",
        "    self.layers = layers\n",
        "    self.O_ALL = None\n",
        "    self.input_shape = input_shape\n",
        "    self.layers[0].initialize(self.input_shape)\n",
        "    for i in range(1, len(self.layers)):\n",
        "      self.layers[i].initialize(self.layers[i-1].n_neurons)\n",
        "  \n",
        "  def predict(self, X):\n",
        "    Oi = X\n",
        "    self.O_ALL = []\n",
        "    self.O_ALL.append(X)\n",
        "    for layer in self.layers:\n",
        "      Oi = layer.compute(Oi)[1]\n",
        "      self.O_ALL.append(Oi)\n",
        "    return Oi\n",
        "  \n",
        "\n",
        "  def loss(self, Oj, y):\n",
        "    E = (2 / y.shape[0]) * np.sum((y - Oj) ** 2)\n",
        "    return E\n",
        "  \n",
        "  def arg(self, y):\n",
        "    predictions = np.zeros((y.shape[0], 1))\n",
        "    for index, instance in enumerate(y):\n",
        "      idx = 0\n",
        "      for i in range(1, 10):\n",
        "        if instance[i] > instance[idx]:\n",
        "          idx = i\n",
        "      predictions[index] = idx\n",
        "    return predictions\n",
        "\n",
        "  def accuracy(self, y_pred, y_true):\n",
        "    return np.sum(np.equal(self.arg(y_true), self.arg(y_pred))) / len(y_true)\n",
        "\n",
        "  def train(self, X_train, y_train, learning_rate, epochs, \n",
        "            X_val = None, y_val = None):\n",
        "    for i in range(epochs):\n",
        "        y_pred = self.predict(X_train)\n",
        "        print(f'epoch {i} accuracy {self.accuracy(y_pred, y_train)}' ,\n",
        "              f'loss {self.loss(y_train, y_pred)}')\n",
        "        for index, instance in enumerate(X_train):\n",
        "          self.predict(instance.reshape(1, -1))\n",
        "          for j in reversed(range(len(self.layers))):\n",
        "            if j == len(self.layers) - 1:\n",
        "              Dw, delta = self.layers[j].grad(y_train[index], self.O_ALL[-2])\n",
        "            else:\n",
        "              Dw, delta = self.layers[j].grad(delta, hold, self.O_ALL[j])\n",
        "            hold = self.layers[j].w.copy()\n",
        "            self.layers[j].update(learning_rate, Dw)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layers2 = [\n",
        "    Layer(10, 'sigmoid'),\n",
        "    Layer(10, 'sigmoid'),\n",
        "    OutputLayer(10, 'sigmoid')\n",
        "]\n",
        "nn2 = NeuralNetwork(layers2, 18)\n",
        "nn2.train(X_train, y_train, 1e-7, 10)"
      ],
      "metadata": {
        "id": "D5-C_1w4BpiK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11a5f270-01a1-4291-d8c7-01dc6687886a"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0 accuracy 0.051 loss 10.606282083284972\n",
            "epoch 1 accuracy 0.051 loss 10.604816549083935\n",
            "epoch 2 accuracy 0.051 loss 10.603350501235964\n",
            "epoch 3 accuracy 0.051 loss 10.601883946034476\n",
            "epoch 4 accuracy 0.051 loss 10.600416889791443\n",
            "epoch 5 accuracy 0.051 loss 10.59894933883722\n",
            "epoch 6 accuracy 0.051 loss 10.597481299519922\n",
            "epoch 7 accuracy 0.051 loss 10.596012778205125\n",
            "epoch 8 accuracy 0.051 loss 10.594543781275409\n",
            "epoch 9 accuracy 0.051 loss 10.593074315130183\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "mbLNbXd8JELY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30217d8d-46bc-430d-863e-532805b80532"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 0 accuracy 0.10706 loss 4973114.190725495\n",
            "iteration 1 accuracy 0.0872 loss 16733.089653797753\n",
            "iteration 2 accuracy 0.0827 loss 8950.565776341364\n",
            "iteration 3 accuracy 0.0806 loss 6120.3890279291\n",
            "iteration 4 accuracy 0.0804 loss 4767.611759023827\n",
            "iteration 5 accuracy 0.0796 loss 3924.412014055008\n",
            "iteration 6 accuracy 0.07954 loss 3333.8984557330405\n",
            "iteration 7 accuracy 0.08008 loss 2894.9111716518937\n",
            "iteration 8 accuracy 0.08018 loss 2556.3578980559487\n",
            "iteration 9 accuracy 0.07998 loss 2288.2295193494356\n",
            "iteration 10 accuracy 0.08116 loss 2069.6612726637954\n",
            "iteration 11 accuracy 0.0822 loss 1888.7495025891533\n",
            "iteration 12 accuracy 0.08366 loss 1736.1106994707134\n",
            "iteration 13 accuracy 0.08444 loss 1605.008249212348\n",
            "iteration 14 accuracy 0.08646 loss 1491.2001874831506\n",
            "iteration 15 accuracy 0.0889 loss 1391.7717067544365\n",
            "iteration 16 accuracy 0.09136 loss 1304.9180143227186\n",
            "iteration 17 accuracy 0.09284 loss 1228.186795884842\n",
            "iteration 18 accuracy 0.09426 loss 1159.7845231318906\n",
            "iteration 19 accuracy 0.09628 loss 1098.4217182231594\n",
            "iteration 20 accuracy 0.09858 loss 1043.0632967096244\n",
            "iteration 21 accuracy 0.10038 loss 992.8549899053019\n",
            "iteration 22 accuracy 0.10204 loss 947.0648519350699\n",
            "iteration 23 accuracy 0.10396 loss 905.1950453135146\n",
            "iteration 24 accuracy 0.10556 loss 866.7776521226473\n",
            "iteration 25 accuracy 0.10728 loss 831.4589026010271\n",
            "iteration 26 accuracy 0.10856 loss 798.9105865589191\n",
            "iteration 27 accuracy 0.10992 loss 768.7366739637707\n",
            "iteration 28 accuracy 0.11126 loss 740.669132859195\n",
            "iteration 29 accuracy 0.11266 loss 714.4930193565345\n",
            "iteration 30 accuracy 0.1141 loss 689.977765466074\n",
            "iteration 31 accuracy 0.11508 loss 666.9752051698958\n",
            "iteration 32 accuracy 0.1161 loss 645.3335609165493\n",
            "iteration 33 accuracy 0.11742 loss 624.8974675215959\n",
            "iteration 34 accuracy 0.11852 loss 605.5640231667406\n",
            "iteration 35 accuracy 0.11968 loss 587.2658666337027\n",
            "iteration 36 accuracy 0.12082 loss 569.9006832223968\n",
            "iteration 37 accuracy 0.122 loss 553.4224729230043\n",
            "iteration 38 accuracy 0.1229 loss 537.7644784816516\n",
            "iteration 39 accuracy 0.12396 loss 522.8575942363353\n",
            "iteration 40 accuracy 0.1248 loss 508.64594486160917\n",
            "iteration 41 accuracy 0.1252 loss 495.09579537507403\n",
            "iteration 42 accuracy 0.12608 loss 482.1599318741884\n",
            "iteration 43 accuracy 0.12678 loss 469.82351848651405\n",
            "iteration 44 accuracy 0.12754 loss 458.0328707812771\n",
            "iteration 45 accuracy 0.12808 loss 446.6940333458054\n",
            "iteration 46 accuracy 0.12882 loss 435.82799849310186\n",
            "iteration 47 accuracy 0.12916 loss 425.42406145030026\n",
            "iteration 48 accuracy 0.12958 loss 415.4431347790944\n",
            "iteration 49 accuracy 0.13 loss 405.8489197137131\n",
            "iteration 50 accuracy 0.13068 loss 396.6371441983725\n",
            "iteration 51 accuracy 0.13142 loss 387.7725296075825\n",
            "iteration 52 accuracy 0.13198 loss 379.2654425498731\n",
            "iteration 53 accuracy 0.13248 loss 371.04067886671686\n",
            "iteration 54 accuracy 0.13302 loss 363.1248222573408\n",
            "iteration 55 accuracy 0.13352 loss 355.47910617224534\n",
            "iteration 56 accuracy 0.13392 loss 348.085329325975\n",
            "iteration 57 accuracy 0.13444 loss 340.97284075988995\n",
            "iteration 58 accuracy 0.13494 loss 334.09977051692925\n",
            "iteration 59 accuracy 0.1358 loss 327.4527961709914\n",
            "iteration 60 accuracy 0.13624 loss 321.0138449836649\n",
            "iteration 61 accuracy 0.13688 loss 314.7957147824424\n",
            "iteration 62 accuracy 0.13724 loss 308.7459169774432\n",
            "iteration 63 accuracy 0.13732 loss 302.9048832850142\n",
            "iteration 64 accuracy 0.13778 loss 297.2426123989274\n",
            "iteration 65 accuracy 0.1382 loss 291.74182080452107\n",
            "iteration 66 accuracy 0.13898 loss 286.38905992569323\n",
            "iteration 67 accuracy 0.13878 loss 281.1744355900792\n",
            "iteration 68 accuracy 0.13896 loss 276.1036712213309\n",
            "iteration 69 accuracy 0.13956 loss 271.1667470955706\n",
            "iteration 70 accuracy 0.1397 loss 266.3566931993329\n",
            "iteration 71 accuracy 0.13992 loss 261.6858114192597\n",
            "iteration 72 accuracy 0.13998 loss 257.14359355216783\n",
            "iteration 73 accuracy 0.14026 loss 252.73105844276586\n",
            "iteration 74 accuracy 0.1405 loss 248.44215666597134\n",
            "iteration 75 accuracy 0.1408 loss 244.25614561414065\n",
            "iteration 76 accuracy 0.14118 loss 240.18392896536855\n",
            "iteration 77 accuracy 0.14148 loss 236.23378956912757\n",
            "iteration 78 accuracy 0.14154 loss 232.3838810688515\n",
            "iteration 79 accuracy 0.1414 loss 228.63616790044227\n",
            "iteration 80 accuracy 0.14154 loss 224.9970600872186\n",
            "iteration 81 accuracy 0.1415 loss 221.45984844402005\n",
            "iteration 82 accuracy 0.1417 loss 218.0123157704741\n",
            "iteration 83 accuracy 0.14178 loss 214.6605522479563\n",
            "iteration 84 accuracy 0.14166 loss 211.39899668220215\n",
            "iteration 85 accuracy 0.14198 loss 208.22534194571952\n",
            "iteration 86 accuracy 0.14224 loss 205.1331222062345\n",
            "iteration 87 accuracy 0.14222 loss 202.11896800457907\n",
            "iteration 88 accuracy 0.14214 loss 199.18033834556056\n",
            "iteration 89 accuracy 0.14216 loss 196.31457952114766\n",
            "iteration 90 accuracy 0.14236 loss 193.51857359867822\n",
            "iteration 91 accuracy 0.14248 loss 190.7982823632136\n",
            "iteration 92 accuracy 0.14256 loss 188.14215519522602\n",
            "iteration 93 accuracy 0.1425 loss 185.55666898605844\n",
            "iteration 94 accuracy 0.14256 loss 183.03479119071005\n",
            "iteration 95 accuracy 0.14248 loss 180.5582134080224\n",
            "iteration 96 accuracy 0.14262 loss 178.15521542732964\n",
            "iteration 97 accuracy 0.14236 loss 175.80904574132188\n",
            "iteration 98 accuracy 0.14246 loss 173.52000641369315\n",
            "iteration 99 accuracy 0.14256 loss 171.28243476882463\n",
            "iteration 100 accuracy 0.14264 loss 169.10063990272124\n",
            "iteration 101 accuracy 0.14262 loss 166.9700276586879\n",
            "iteration 102 accuracy 0.14246 loss 164.88799376701192\n",
            "iteration 103 accuracy 0.1423 loss 162.85155891586433\n",
            "iteration 104 accuracy 0.1422 loss 160.85929344712432\n",
            "iteration 105 accuracy 0.1418 loss 158.91332250593356\n",
            "iteration 106 accuracy 0.14178 loss 157.00954856517802\n",
            "iteration 107 accuracy 0.1416 loss 155.1475244758579\n",
            "iteration 108 accuracy 0.14144 loss 153.32793226818558\n",
            "iteration 109 accuracy 0.14144 loss 151.5465325472863\n",
            "iteration 110 accuracy 0.14148 loss 149.8032423658744\n",
            "iteration 111 accuracy 0.1414 loss 148.0968505440394\n",
            "iteration 112 accuracy 0.14132 loss 146.42160603272487\n",
            "iteration 113 accuracy 0.14132 loss 144.7804097643768\n",
            "iteration 114 accuracy 0.14122 loss 143.17236270143914\n",
            "iteration 115 accuracy 0.14126 loss 141.5946188460457\n",
            "iteration 116 accuracy 0.14124 loss 140.04805157388\n",
            "iteration 117 accuracy 0.14136 loss 138.53119117122696\n",
            "iteration 118 accuracy 0.14128 loss 137.04448040144214\n",
            "iteration 119 accuracy 0.14146 loss 135.58225081081355\n",
            "iteration 120 accuracy 0.14142 loss 134.14844321882106\n",
            "iteration 121 accuracy 0.14114 loss 132.74230458964473\n",
            "iteration 122 accuracy 0.14106 loss 131.35967909351746\n",
            "iteration 123 accuracy 0.14096 loss 130.0044418233\n",
            "iteration 124 accuracy 0.14082 loss 128.67608990420672\n",
            "iteration 125 accuracy 0.14088 loss 127.37802572036217\n",
            "iteration 126 accuracy 0.14076 loss 126.09941316176773\n",
            "iteration 127 accuracy 0.14064 loss 124.8440405964465\n",
            "iteration 128 accuracy 0.1406 loss 123.6062877589009\n",
            "iteration 129 accuracy 0.1406 loss 122.39237525625109\n",
            "iteration 130 accuracy 0.14052 loss 121.20085188833028\n",
            "iteration 131 accuracy 0.14064 loss 120.02922997439242\n",
            "iteration 132 accuracy 0.14034 loss 118.87509056418064\n",
            "iteration 133 accuracy 0.14042 loss 117.73515850267026\n",
            "iteration 134 accuracy 0.14054 loss 116.62388304613653\n",
            "iteration 135 accuracy 0.1404 loss 115.53237605055371\n",
            "iteration 136 accuracy 0.14046 loss 114.46067937790627\n",
            "iteration 137 accuracy 0.14044 loss 113.40729835799475\n",
            "iteration 138 accuracy 0.1405 loss 112.36624141003448\n",
            "iteration 139 accuracy 0.14054 loss 111.3427761250427\n",
            "iteration 140 accuracy 0.14038 loss 110.33502614921147\n",
            "iteration 141 accuracy 0.14058 loss 109.34379917996345\n",
            "iteration 142 accuracy 0.14074 loss 108.3662754601733\n",
            "iteration 143 accuracy 0.14066 loss 107.40330505880208\n",
            "iteration 144 accuracy 0.14062 loss 106.45831232337974\n",
            "iteration 145 accuracy 0.14058 loss 105.5290343671433\n",
            "iteration 146 accuracy 0.1404 loss 104.61377095602776\n",
            "iteration 147 accuracy 0.14042 loss 103.71338169010653\n",
            "iteration 148 accuracy 0.14088 loss 102.8278690019949\n",
            "iteration 149 accuracy 0.1406 loss 101.94518486706684\n",
            "iteration 150 accuracy 0.14058 loss 101.08108986120875\n",
            "iteration 151 accuracy 0.14106 loss 100.22988642263992\n",
            "iteration 152 accuracy 0.14126 loss 99.3933248586029\n",
            "iteration 153 accuracy 0.14154 loss 98.56329169103583\n",
            "iteration 154 accuracy 0.14154 loss 97.74812445803697\n",
            "iteration 155 accuracy 0.1417 loss 96.94574325550697\n",
            "iteration 156 accuracy 0.14176 loss 96.15575570014705\n",
            "iteration 157 accuracy 0.14196 loss 95.38208418448717\n",
            "iteration 158 accuracy 0.1419 loss 94.6116690346714\n",
            "iteration 159 accuracy 0.14214 loss 93.85172716918332\n",
            "iteration 160 accuracy 0.14238 loss 93.10257381352174\n",
            "iteration 161 accuracy 0.14262 loss 92.36307848350317\n",
            "iteration 162 accuracy 0.14306 loss 91.63471732712085\n",
            "iteration 163 accuracy 0.1433 loss 90.9152294340824\n",
            "iteration 164 accuracy 0.14318 loss 90.20067370444858\n",
            "iteration 165 accuracy 0.14352 loss 89.49691315833864\n",
            "iteration 166 accuracy 0.1436 loss 88.80130430976476\n",
            "iteration 167 accuracy 0.14368 loss 88.11246162489547\n",
            "iteration 168 accuracy 0.1439 loss 87.43144831875831\n",
            "iteration 169 accuracy 0.1441 loss 86.75932534703207\n",
            "iteration 170 accuracy 0.14444 loss 86.09571421359381\n",
            "iteration 171 accuracy 0.14462 loss 85.43899771531477\n",
            "iteration 172 accuracy 0.14474 loss 84.79085794705928\n",
            "iteration 173 accuracy 0.145 loss 84.14821404579612\n",
            "iteration 174 accuracy 0.14528 loss 83.51096166877568\n",
            "iteration 175 accuracy 0.1456 loss 82.8814297990775\n",
            "iteration 176 accuracy 0.1459 loss 82.25823921722349\n",
            "iteration 177 accuracy 0.14596 loss 81.64162430164774\n",
            "iteration 178 accuracy 0.14606 loss 81.03196226280198\n",
            "iteration 179 accuracy 0.1462 loss 80.43012088694105\n",
            "iteration 180 accuracy 0.1461 loss 79.83361210163349\n",
            "iteration 181 accuracy 0.14614 loss 79.24327089814695\n",
            "iteration 182 accuracy 0.14592 loss 78.65903124513446\n",
            "iteration 183 accuracy 0.14594 loss 78.08197867703721\n",
            "iteration 184 accuracy 0.14576 loss 77.51142127325778\n",
            "iteration 185 accuracy 0.14592 loss 76.94678100766862\n",
            "iteration 186 accuracy 0.1464 loss 76.39259931918181\n",
            "iteration 187 accuracy 0.14642 loss 75.84133835686401\n",
            "iteration 188 accuracy 0.14648 loss 75.2951542468904\n",
            "iteration 189 accuracy 0.1466 loss 74.75856987628195\n",
            "iteration 190 accuracy 0.14646 loss 74.2270928998043\n",
            "iteration 191 accuracy 0.14644 loss 73.70135281373035\n",
            "iteration 192 accuracy 0.14644 loss 73.18188474791688\n",
            "iteration 193 accuracy 0.1464 loss 72.66740120082082\n",
            "iteration 194 accuracy 0.14626 loss 72.16062415166137\n",
            "iteration 195 accuracy 0.14612 loss 71.65966778392179\n",
            "iteration 196 accuracy 0.14616 loss 71.16409446135951\n",
            "iteration 197 accuracy 0.1461 loss 70.67369014966913\n",
            "iteration 198 accuracy 0.1464 loss 70.18856583436472\n",
            "iteration 199 accuracy 0.14652 loss 69.71103283629634\n",
            "iteration 200 accuracy 0.14644 loss 69.23827372656694\n",
            "iteration 201 accuracy 0.14636 loss 68.77052887948463\n",
            "iteration 202 accuracy 0.14652 loss 68.30930839090206\n",
            "iteration 203 accuracy 0.14662 loss 67.85518466638429\n",
            "iteration 204 accuracy 0.14666 loss 67.40570165572335\n",
            "iteration 205 accuracy 0.14658 loss 66.96062842294872\n",
            "iteration 206 accuracy 0.14636 loss 66.51585989462313\n",
            "iteration 207 accuracy 0.1467 loss 66.08069346220023\n",
            "iteration 208 accuracy 0.14664 loss 65.64999751223489\n",
            "iteration 209 accuracy 0.14668 loss 65.22519668411441\n",
            "iteration 210 accuracy 0.1466 loss 64.80498797385404\n",
            "iteration 211 accuracy 0.14656 loss 64.38891011368362\n",
            "iteration 212 accuracy 0.14664 loss 63.97705084492996\n",
            "iteration 213 accuracy 0.14662 loss 63.56974968422527\n",
            "iteration 214 accuracy 0.1466 loss 63.16703723519382\n",
            "iteration 215 accuracy 0.1467 loss 62.768511400814106\n",
            "iteration 216 accuracy 0.14688 loss 62.37589353528977\n",
            "iteration 217 accuracy 0.14692 loss 61.988250816279866\n",
            "iteration 218 accuracy 0.1471 loss 61.60496553110039\n",
            "iteration 219 accuracy 0.1474 loss 61.22615842287766\n",
            "iteration 220 accuracy 0.14728 loss 60.85378864025532\n",
            "iteration 221 accuracy 0.14752 loss 60.48519588616914\n",
            "iteration 222 accuracy 0.14776 loss 60.12142095742382\n",
            "iteration 223 accuracy 0.14812 loss 59.76124400587442\n",
            "iteration 224 accuracy 0.1482 loss 59.405402125464526\n",
            "iteration 225 accuracy 0.1484 loss 59.053212700828205\n",
            "iteration 226 accuracy 0.14894 loss 58.70476637140468\n",
            "iteration 227 accuracy 0.14902 loss 58.359660255647775\n",
            "iteration 228 accuracy 0.14922 loss 58.01774752674972\n",
            "iteration 229 accuracy 0.1497 loss 57.6794831645646\n",
            "iteration 230 accuracy 0.14994 loss 57.344965370844605\n",
            "iteration 231 accuracy 0.15022 loss 57.014439608965304\n",
            "iteration 232 accuracy 0.15028 loss 56.68830329769354\n",
            "iteration 233 accuracy 0.15048 loss 56.36431608261256\n",
            "iteration 234 accuracy 0.15062 loss 56.043249165590446\n",
            "iteration 235 accuracy 0.1508 loss 55.72201674498706\n",
            "iteration 236 accuracy 0.15102 loss 55.40644448598778\n",
            "iteration 237 accuracy 0.15138 loss 55.09438714938238\n",
            "iteration 238 accuracy 0.15132 loss 54.78536368753481\n",
            "iteration 239 accuracy 0.15162 loss 54.47786064833739\n",
            "iteration 240 accuracy 0.1516 loss 54.17191867846191\n",
            "iteration 241 accuracy 0.15188 loss 53.86969421522766\n",
            "iteration 242 accuracy 0.15194 loss 53.57119883923027\n",
            "iteration 243 accuracy 0.15202 loss 53.27603600493492\n",
            "iteration 244 accuracy 0.1523 loss 52.987680225315806\n",
            "iteration 245 accuracy 0.15242 loss 52.698153895237674\n",
            "iteration 246 accuracy 0.15284 loss 52.41178075428882\n",
            "iteration 247 accuracy 0.15292 loss 52.128967420439274\n",
            "iteration 248 accuracy 0.15314 loss 51.849122148606114\n",
            "iteration 249 accuracy 0.15324 loss 51.57256142193537\n",
            "iteration 250 accuracy 0.1532 loss 51.29861611492991\n",
            "iteration 251 accuracy 0.15322 loss 51.02740755292186\n",
            "iteration 252 accuracy 0.15322 loss 50.7592517215453\n",
            "iteration 253 accuracy 0.1536 loss 50.49399118998734\n",
            "iteration 254 accuracy 0.15372 loss 50.231241830398595\n",
            "iteration 255 accuracy 0.1538 loss 49.97105455297528\n",
            "iteration 256 accuracy 0.15402 loss 49.712408418228556\n",
            "iteration 257 accuracy 0.15416 loss 49.456378563515976\n",
            "iteration 258 accuracy 0.15436 loss 49.203381753709834\n",
            "iteration 259 accuracy 0.15442 loss 48.95303258006736\n",
            "iteration 260 accuracy 0.15474 loss 48.704716036768616\n",
            "iteration 261 accuracy 0.15484 loss 48.45713583920492\n",
            "iteration 262 accuracy 0.155 loss 48.21346965408368\n",
            "iteration 263 accuracy 0.1552 loss 47.97226003247496\n",
            "iteration 264 accuracy 0.15506 loss 47.73231323496124\n",
            "iteration 265 accuracy 0.15536 loss 47.49424424281145\n",
            "iteration 266 accuracy 0.15556 loss 47.25833745951796\n",
            "iteration 267 accuracy 0.15558 loss 47.02367930414951\n",
            "iteration 268 accuracy 0.15554 loss 46.79084919273769\n",
            "iteration 269 accuracy 0.1556 loss 46.56019371499633\n",
            "iteration 270 accuracy 0.15574 loss 46.332047140193886\n",
            "iteration 271 accuracy 0.15568 loss 46.104222405545976\n",
            "iteration 272 accuracy 0.15584 loss 45.87932153593657\n",
            "iteration 273 accuracy 0.156 loss 45.65683443626838\n",
            "iteration 274 accuracy 0.15634 loss 45.43944548237337\n",
            "iteration 275 accuracy 0.15636 loss 45.218841425840935\n",
            "iteration 276 accuracy 0.15654 loss 44.999912037787105\n",
            "iteration 277 accuracy 0.15678 loss 44.78464161721966\n",
            "iteration 278 accuracy 0.15702 loss 44.56863212982204\n",
            "iteration 279 accuracy 0.1575 loss 44.35463416941048\n",
            "iteration 280 accuracy 0.15762 loss 44.14220161715919\n",
            "iteration 281 accuracy 0.15784 loss 43.93076544581272\n",
            "iteration 282 accuracy 0.15818 loss 43.721261735967474\n",
            "iteration 283 accuracy 0.15842 loss 43.51216945187325\n",
            "iteration 284 accuracy 0.15868 loss 43.30549439148796\n",
            "iteration 285 accuracy 0.15906 loss 43.09907231683163\n",
            "iteration 286 accuracy 0.15928 loss 42.893526465229044\n",
            "iteration 287 accuracy 0.15952 loss 42.68842852352843\n",
            "iteration 288 accuracy 0.1598 loss 42.486046276910585\n",
            "iteration 289 accuracy 0.1602 loss 42.28520517150085\n",
            "iteration 290 accuracy 0.16036 loss 42.08554605954582\n",
            "iteration 291 accuracy 0.16064 loss 41.88738665982561\n",
            "iteration 292 accuracy 0.16078 loss 41.691053355516104\n",
            "iteration 293 accuracy 0.16092 loss 41.496867266981795\n",
            "iteration 294 accuracy 0.16108 loss 41.30428714696856\n",
            "iteration 295 accuracy 0.16144 loss 41.113404212424385\n",
            "iteration 296 accuracy 0.16162 loss 40.91867148670857\n",
            "iteration 297 accuracy 0.16182 loss 40.73062261138164\n",
            "iteration 298 accuracy 0.16206 loss 40.543512164762504\n",
            "iteration 299 accuracy 0.1622 loss 40.35769316297257\n",
            "iteration 300 accuracy 0.16244 loss 40.17420404815945\n",
            "iteration 301 accuracy 0.16278 loss 39.991520315886085\n",
            "iteration 302 accuracy 0.16306 loss 39.81060482313995\n",
            "iteration 303 accuracy 0.16316 loss 39.63091154552699\n",
            "iteration 304 accuracy 0.16328 loss 39.4516367770453\n",
            "iteration 305 accuracy 0.1634 loss 39.2734146306894\n",
            "iteration 306 accuracy 0.16346 loss 39.09690566521231\n",
            "iteration 307 accuracy 0.16378 loss 38.92215411508581\n",
            "iteration 308 accuracy 0.16418 loss 38.751419988734355\n",
            "iteration 309 accuracy 0.1645 loss 38.58016182072501\n",
            "iteration 310 accuracy 0.16474 loss 38.41024812049135\n",
            "iteration 311 accuracy 0.16508 loss 38.241998296728596\n",
            "iteration 312 accuracy 0.16544 loss 38.07520899657708\n",
            "iteration 313 accuracy 0.16572 loss 37.91014202075968\n",
            "iteration 314 accuracy 0.16616 loss 37.746598852880346\n",
            "iteration 315 accuracy 0.16652 loss 37.58440800611877\n",
            "iteration 316 accuracy 0.16664 loss 37.423601352429266\n",
            "iteration 317 accuracy 0.16686 loss 37.264371361235\n",
            "iteration 318 accuracy 0.16712 loss 37.1066878318731\n",
            "iteration 319 accuracy 0.16756 loss 36.9503396191841\n",
            "iteration 320 accuracy 0.1679 loss 36.79494362078002\n",
            "iteration 321 accuracy 0.1682 loss 36.640846278870946\n",
            "iteration 322 accuracy 0.16832 loss 36.48793594985969\n",
            "iteration 323 accuracy 0.1687 loss 36.33584130567252\n",
            "iteration 324 accuracy 0.16902 loss 36.185073146896016\n",
            "iteration 325 accuracy 0.1695 loss 36.0355520374191\n",
            "iteration 326 accuracy 0.16974 loss 35.88719057269146\n",
            "iteration 327 accuracy 0.17012 loss 35.74024993002589\n",
            "iteration 328 accuracy 0.17068 loss 35.5944402862224\n",
            "iteration 329 accuracy 0.171 loss 35.45002975009209\n",
            "iteration 330 accuracy 0.17132 loss 35.30629054583721\n",
            "iteration 331 accuracy 0.17156 loss 35.16376158250474\n",
            "iteration 332 accuracy 0.1718 loss 35.02228484302368\n",
            "iteration 333 accuracy 0.17232 loss 34.881889560882556\n",
            "iteration 334 accuracy 0.17278 loss 34.742040165579986\n",
            "iteration 335 accuracy 0.173 loss 34.60336970084011\n",
            "iteration 336 accuracy 0.17336 loss 34.46604618989252\n",
            "iteration 337 accuracy 0.17356 loss 34.33017036769058\n",
            "iteration 338 accuracy 0.17388 loss 34.19549702265016\n",
            "iteration 339 accuracy 0.17402 loss 34.062004267605644\n",
            "iteration 340 accuracy 0.1743 loss 33.92967233978866\n",
            "iteration 341 accuracy 0.17462 loss 33.798376395385645\n",
            "iteration 342 accuracy 0.17496 loss 33.668676295740944\n",
            "iteration 343 accuracy 0.17512 loss 33.53935701851569\n",
            "iteration 344 accuracy 0.17544 loss 33.410712845823134\n",
            "iteration 345 accuracy 0.17558 loss 33.28277362373289\n",
            "iteration 346 accuracy 0.17584 loss 33.15569100742847\n",
            "iteration 347 accuracy 0.17616 loss 33.029365887383115\n",
            "iteration 348 accuracy 0.17654 loss 32.90389242089555\n",
            "iteration 349 accuracy 0.17686 loss 32.779383742709136\n",
            "iteration 350 accuracy 0.1771 loss 32.65567273089548\n",
            "iteration 351 accuracy 0.1774 loss 32.53287820618333\n",
            "iteration 352 accuracy 0.17776 loss 32.41089983741303\n",
            "iteration 353 accuracy 0.17802 loss 32.28980620774196\n",
            "iteration 354 accuracy 0.1782 loss 32.16954784579724\n",
            "iteration 355 accuracy 0.17852 loss 32.05015968139287\n",
            "iteration 356 accuracy 0.17884 loss 31.93114589947475\n",
            "iteration 357 accuracy 0.179 loss 31.813125874507676\n",
            "iteration 358 accuracy 0.17914 loss 31.695921068028532\n",
            "iteration 359 accuracy 0.17952 loss 31.578640411916545\n",
            "iteration 360 accuracy 0.17968 loss 31.462361132645928\n",
            "iteration 361 accuracy 0.17988 loss 31.34624760115999\n",
            "iteration 362 accuracy 0.18032 loss 31.229973106860953\n",
            "iteration 363 accuracy 0.18048 loss 31.114541528491404\n",
            "iteration 364 accuracy 0.18078 loss 31.000229362843072\n",
            "iteration 365 accuracy 0.18114 loss 30.886841133369234\n",
            "iteration 366 accuracy 0.18134 loss 30.773943970468753\n",
            "iteration 367 accuracy 0.18158 loss 30.6614476688825\n",
            "iteration 368 accuracy 0.18188 loss 30.550092103721042\n",
            "iteration 369 accuracy 0.18218 loss 30.439453559061274\n",
            "iteration 370 accuracy 0.18248 loss 30.329774392495125\n",
            "iteration 371 accuracy 0.18266 loss 30.220379909126216\n",
            "iteration 372 accuracy 0.18348 loss 30.112467734557967\n",
            "iteration 373 accuracy 0.18368 loss 30.004821539980075\n",
            "iteration 374 accuracy 0.18392 loss 29.89796626974441\n",
            "iteration 375 accuracy 0.18406 loss 29.79188357426903\n",
            "iteration 376 accuracy 0.18432 loss 29.6863291779657\n",
            "iteration 377 accuracy 0.1846 loss 29.581230571713856\n",
            "iteration 378 accuracy 0.1848 loss 29.477047032399298\n",
            "iteration 379 accuracy 0.18508 loss 29.373338100197252\n",
            "iteration 380 accuracy 0.18522 loss 29.269939502362135\n",
            "iteration 381 accuracy 0.1855 loss 29.166896255056812\n",
            "iteration 382 accuracy 0.18584 loss 29.064613910596645\n",
            "iteration 383 accuracy 0.18594 loss 28.9632566271108\n",
            "iteration 384 accuracy 0.18616 loss 28.862508701033622\n",
            "iteration 385 accuracy 0.18648 loss 28.762153035110227\n",
            "iteration 386 accuracy 0.18674 loss 28.662490380509567\n",
            "iteration 387 accuracy 0.1872 loss 28.563521666495124\n",
            "iteration 388 accuracy 0.1873 loss 28.46442231336586\n",
            "iteration 389 accuracy 0.18756 loss 28.364984227230096\n",
            "iteration 390 accuracy 0.18772 loss 28.26491239235154\n",
            "iteration 391 accuracy 0.1881 loss 28.163484504292608\n",
            "iteration 392 accuracy 0.18848 loss 28.062581626640146\n",
            "iteration 393 accuracy 0.18868 loss 27.963134885442305\n",
            "iteration 394 accuracy 0.18902 loss 27.86430689143555\n",
            "iteration 395 accuracy 0.18924 loss 27.766729254859918\n",
            "iteration 396 accuracy 0.1896 loss 27.669562020859473\n",
            "iteration 397 accuracy 0.18986 loss 27.573005918906702\n",
            "iteration 398 accuracy 0.19024 loss 27.477692204931884\n",
            "iteration 399 accuracy 0.19044 loss 27.383192849751605\n",
            "iteration 400 accuracy 0.19066 loss 27.289469259567138\n",
            "iteration 401 accuracy 0.19092 loss 27.19563181019509\n",
            "iteration 402 accuracy 0.19122 loss 27.10247325039956\n",
            "iteration 403 accuracy 0.19162 loss 27.010184616636234\n",
            "iteration 404 accuracy 0.19174 loss 26.918344563787027\n",
            "iteration 405 accuracy 0.19192 loss 26.826504068953785\n",
            "iteration 406 accuracy 0.19216 loss 26.735366963816404\n",
            "iteration 407 accuracy 0.19236 loss 26.644884504486217\n",
            "iteration 408 accuracy 0.19262 loss 26.55475299407823\n",
            "iteration 409 accuracy 0.19274 loss 26.465411088686402\n",
            "iteration 410 accuracy 0.193 loss 26.37659165381402\n",
            "iteration 411 accuracy 0.19314 loss 26.288434233371234\n",
            "iteration 412 accuracy 0.19338 loss 26.200151518728227\n",
            "iteration 413 accuracy 0.1937 loss 26.112373073467246\n",
            "iteration 414 accuracy 0.19412 loss 26.02452710663783\n",
            "iteration 415 accuracy 0.19436 loss 25.93694399121969\n",
            "iteration 416 accuracy 0.19462 loss 25.84993110603289\n",
            "iteration 417 accuracy 0.19494 loss 25.76353704911411\n",
            "iteration 418 accuracy 0.19516 loss 25.677352184937124\n",
            "iteration 419 accuracy 0.1955 loss 25.591848450527195\n",
            "iteration 420 accuracy 0.19572 loss 25.507497628486032\n",
            "iteration 421 accuracy 0.1959 loss 25.422976725954523\n",
            "iteration 422 accuracy 0.19622 loss 25.33874554183873\n",
            "iteration 423 accuracy 0.19646 loss 25.25471050047871\n",
            "iteration 424 accuracy 0.19684 loss 25.17113334738447\n",
            "iteration 425 accuracy 0.19714 loss 25.088005594108573\n",
            "iteration 426 accuracy 0.1974 loss 25.005276904698416\n",
            "iteration 427 accuracy 0.1978 loss 24.92308340606575\n",
            "iteration 428 accuracy 0.19806 loss 24.841797927114527\n",
            "iteration 429 accuracy 0.1983 loss 24.760265877457623\n",
            "iteration 430 accuracy 0.19844 loss 24.678502675394714\n",
            "iteration 431 accuracy 0.19872 loss 24.596963598599114\n",
            "iteration 432 accuracy 0.19904 loss 24.515907631403266\n",
            "iteration 433 accuracy 0.19934 loss 24.435406994387627\n",
            "iteration 434 accuracy 0.19978 loss 24.355307847052927\n",
            "iteration 435 accuracy 0.2001 loss 24.27556430154255\n",
            "iteration 436 accuracy 0.20032 loss 24.196116420075047\n",
            "iteration 437 accuracy 0.20086 loss 24.11675977377975\n",
            "iteration 438 accuracy 0.20134 loss 24.037896707598463\n",
            "iteration 439 accuracy 0.20146 loss 23.95971681523535\n",
            "iteration 440 accuracy 0.20184 loss 23.881530344972333\n",
            "iteration 441 accuracy 0.2021 loss 23.803505694740394\n",
            "iteration 442 accuracy 0.2024 loss 23.726198423747437\n",
            "iteration 443 accuracy 0.20274 loss 23.64931459836555\n",
            "iteration 444 accuracy 0.20312 loss 23.572453087858705\n",
            "iteration 445 accuracy 0.20344 loss 23.496047090693377\n",
            "iteration 446 accuracy 0.20378 loss 23.42007728536186\n",
            "iteration 447 accuracy 0.20398 loss 23.344688796210907\n",
            "iteration 448 accuracy 0.20414 loss 23.26943418629559\n",
            "iteration 449 accuracy 0.2042 loss 23.19505416488292\n",
            "iteration 450 accuracy 0.20442 loss 23.121181083687286\n",
            "iteration 451 accuracy 0.20468 loss 23.04767933889492\n",
            "iteration 452 accuracy 0.20496 loss 22.974593023067083\n",
            "iteration 453 accuracy 0.2052 loss 22.901957998364463\n",
            "iteration 454 accuracy 0.2055 loss 22.829701171702187\n",
            "iteration 455 accuracy 0.20558 loss 22.75785702358064\n",
            "iteration 456 accuracy 0.20564 loss 22.686495125633392\n",
            "iteration 457 accuracy 0.20598 loss 22.615112566770726\n",
            "iteration 458 accuracy 0.2062 loss 22.54399040303539\n",
            "iteration 459 accuracy 0.20658 loss 22.473442364340183\n",
            "iteration 460 accuracy 0.20682 loss 22.403242753002456\n",
            "iteration 461 accuracy 0.20706 loss 22.33379627859278\n",
            "iteration 462 accuracy 0.20732 loss 22.264658582867888\n",
            "iteration 463 accuracy 0.20752 loss 22.196063314022368\n",
            "iteration 464 accuracy 0.20776 loss 22.12786272477331\n",
            "iteration 465 accuracy 0.20806 loss 22.060108722743315\n",
            "iteration 466 accuracy 0.20838 loss 21.99275327595221\n",
            "iteration 467 accuracy 0.20886 loss 21.926027455525084\n",
            "iteration 468 accuracy 0.20904 loss 21.859494327847802\n",
            "iteration 469 accuracy 0.20936 loss 21.793776111294388\n",
            "iteration 470 accuracy 0.2097 loss 21.727840561343516\n",
            "iteration 471 accuracy 0.20994 loss 21.662286477233653\n",
            "iteration 472 accuracy 0.21014 loss 21.59714303716241\n",
            "iteration 473 accuracy 0.21032 loss 21.53195566261152\n",
            "iteration 474 accuracy 0.21064 loss 21.467390878445737\n",
            "iteration 475 accuracy 0.21084 loss 21.403291083841875\n",
            "iteration 476 accuracy 0.21104 loss 21.33963308974594\n",
            "iteration 477 accuracy 0.21144 loss 21.27641101154893\n",
            "iteration 478 accuracy 0.21172 loss 21.213685186145273\n",
            "iteration 479 accuracy 0.21206 loss 21.151328732661263\n",
            "iteration 480 accuracy 0.2124 loss 21.089024144416435\n",
            "iteration 481 accuracy 0.21254 loss 21.027092618405597\n",
            "iteration 482 accuracy 0.21284 loss 20.965613358194773\n",
            "iteration 483 accuracy 0.21302 loss 20.904570665899456\n",
            "iteration 484 accuracy 0.21342 loss 20.843640375402337\n",
            "iteration 485 accuracy 0.2136 loss 20.783099356364257\n",
            "iteration 486 accuracy 0.21394 loss 20.722847825444116\n",
            "iteration 487 accuracy 0.21414 loss 20.662910927461862\n",
            "iteration 488 accuracy 0.21434 loss 20.603292452979503\n",
            "iteration 489 accuracy 0.21456 loss 20.54402587543855\n",
            "iteration 490 accuracy 0.21488 loss 20.485474478874966\n",
            "iteration 491 accuracy 0.21508 loss 20.426320453990915\n",
            "iteration 492 accuracy 0.21534 loss 20.367424105678047\n",
            "iteration 493 accuracy 0.21566 loss 20.30891584719844\n",
            "iteration 494 accuracy 0.21558 loss 20.24912215134166\n",
            "iteration 495 accuracy 0.21586 loss 20.191011427210633\n",
            "iteration 496 accuracy 0.21614 loss 20.133269010147064\n",
            "iteration 497 accuracy 0.21642 loss 20.075628561669312\n",
            "iteration 498 accuracy 0.21662 loss 20.018422865027954\n",
            "iteration 499 accuracy 0.21678 loss 19.959530618912535\n",
            "iteration 500 accuracy 0.21698 loss 19.902716121185147\n",
            "iteration 501 accuracy 0.21736 loss 19.845766450492476\n",
            "iteration 502 accuracy 0.21786 loss 19.7894127764223\n",
            "iteration 503 accuracy 0.21804 loss 19.733513614746105\n",
            "iteration 504 accuracy 0.21826 loss 19.678001923054506\n",
            "iteration 505 accuracy 0.21858 loss 19.62286670517178\n",
            "iteration 506 accuracy 0.21902 loss 19.567853977562997\n",
            "iteration 507 accuracy 0.21918 loss 19.513070360815565\n",
            "iteration 508 accuracy 0.21948 loss 19.458562019063717\n",
            "iteration 509 accuracy 0.22 loss 19.40478379738997\n",
            "iteration 510 accuracy 0.22026 loss 19.350845082389498\n",
            "iteration 511 accuracy 0.2206 loss 19.297199756475525\n",
            "iteration 512 accuracy 0.22096 loss 19.24389082451724\n",
            "iteration 513 accuracy 0.221 loss 19.190911914693938\n",
            "iteration 514 accuracy 0.22132 loss 19.13821814348884\n",
            "iteration 515 accuracy 0.22168 loss 19.085870750389873\n",
            "iteration 516 accuracy 0.22198 loss 19.033645616098177\n",
            "iteration 517 accuracy 0.22222 loss 18.981764264508232\n",
            "iteration 518 accuracy 0.22244 loss 18.930067753823025\n",
            "iteration 519 accuracy 0.22272 loss 18.878418515146958\n",
            "iteration 520 accuracy 0.22292 loss 18.827339748469207\n",
            "iteration 521 accuracy 0.22314 loss 18.77656798711129\n",
            "iteration 522 accuracy 0.22346 loss 18.72579851648373\n",
            "iteration 523 accuracy 0.22372 loss 18.67488993799431\n",
            "iteration 524 accuracy 0.224 loss 18.624272753508496\n",
            "iteration 525 accuracy 0.22416 loss 18.57390128923332\n",
            "iteration 526 accuracy 0.22438 loss 18.52402147043123\n",
            "iteration 527 accuracy 0.22464 loss 18.474117800003558\n",
            "iteration 528 accuracy 0.22488 loss 18.424044021275577\n",
            "iteration 529 accuracy 0.22502 loss 18.374184693389807\n",
            "iteration 530 accuracy 0.22532 loss 18.32466613277482\n",
            "iteration 531 accuracy 0.2256 loss 18.2753112203551\n",
            "iteration 532 accuracy 0.22606 loss 18.226151951597043\n",
            "iteration 533 accuracy 0.22634 loss 18.177262435330395\n",
            "iteration 534 accuracy 0.22664 loss 18.128568843945757\n",
            "iteration 535 accuracy 0.22678 loss 18.08011409018848\n",
            "iteration 536 accuracy 0.227 loss 18.03199770458921\n",
            "iteration 537 accuracy 0.22718 loss 17.984183442623028\n",
            "iteration 538 accuracy 0.22734 loss 17.936698344219188\n",
            "iteration 539 accuracy 0.22768 loss 17.889493687880034\n",
            "iteration 540 accuracy 0.2279 loss 17.842588534006396\n",
            "iteration 541 accuracy 0.2283 loss 17.795960614101993\n",
            "iteration 542 accuracy 0.22862 loss 17.749556001915682\n",
            "iteration 543 accuracy 0.2288 loss 17.703373893335122\n",
            "iteration 544 accuracy 0.22938 loss 17.657560932315654\n",
            "iteration 545 accuracy 0.22968 loss 17.611986632478995\n",
            "iteration 546 accuracy 0.23002 loss 17.56649941176161\n",
            "iteration 547 accuracy 0.23024 loss 17.521019299861837\n",
            "iteration 548 accuracy 0.23046 loss 17.475671893199515\n",
            "iteration 549 accuracy 0.23078 loss 17.430525470603335\n",
            "iteration 550 accuracy 0.23098 loss 17.385636587923866\n",
            "iteration 551 accuracy 0.23126 loss 17.341077857515018\n",
            "iteration 552 accuracy 0.2318 loss 17.296408889904907\n",
            "iteration 553 accuracy 0.23188 loss 17.252099309573246\n",
            "iteration 554 accuracy 0.23226 loss 17.207922108574806\n",
            "iteration 555 accuracy 0.23318 loss 17.16447820497087\n",
            "iteration 556 accuracy 0.23336 loss 17.120644518360848\n",
            "iteration 557 accuracy 0.23352 loss 17.077000786370377\n",
            "iteration 558 accuracy 0.23352 loss 17.033441951740247\n",
            "iteration 559 accuracy 0.23372 loss 16.99019290896871\n",
            "iteration 560 accuracy 0.23406 loss 16.947226746053392\n",
            "iteration 561 accuracy 0.2342 loss 16.904528627620095\n",
            "iteration 562 accuracy 0.23444 loss 16.86205701523812\n",
            "iteration 563 accuracy 0.23468 loss 16.819679183647413\n",
            "iteration 564 accuracy 0.23482 loss 16.777371513963836\n",
            "iteration 565 accuracy 0.23502 loss 16.73545112171806\n",
            "iteration 566 accuracy 0.23522 loss 16.693558094039354\n",
            "iteration 567 accuracy 0.23528 loss 16.651921019062044\n",
            "iteration 568 accuracy 0.23554 loss 16.6102661652545\n",
            "iteration 569 accuracy 0.23574 loss 16.56890687516612\n",
            "iteration 570 accuracy 0.23602 loss 16.527744008216285\n",
            "iteration 571 accuracy 0.2364 loss 16.486837716686388\n",
            "iteration 572 accuracy 0.23658 loss 16.446059701687044\n",
            "iteration 573 accuracy 0.23696 loss 16.405560281678294\n",
            "iteration 574 accuracy 0.23726 loss 16.36531629639544\n",
            "iteration 575 accuracy 0.23746 loss 16.325208689537792\n",
            "iteration 576 accuracy 0.2376 loss 16.285335484678598\n",
            "iteration 577 accuracy 0.23794 loss 16.245649041684533\n",
            "iteration 578 accuracy 0.23812 loss 16.206010982684415\n",
            "iteration 579 accuracy 0.23826 loss 16.16650498252408\n",
            "iteration 580 accuracy 0.23856 loss 16.126936774822557\n",
            "iteration 581 accuracy 0.23868 loss 16.08763037916773\n",
            "iteration 582 accuracy 0.23882 loss 16.048305819081676\n",
            "iteration 583 accuracy 0.23908 loss 16.009079246060985\n",
            "iteration 584 accuracy 0.23936 loss 15.969963703443936\n",
            "iteration 585 accuracy 0.23972 loss 15.931040039564163\n",
            "iteration 586 accuracy 0.23988 loss 15.89204719869459\n",
            "iteration 587 accuracy 0.23986 loss 15.853527569352275\n",
            "iteration 588 accuracy 0.24014 loss 15.8153293516061\n",
            "iteration 589 accuracy 0.2402 loss 15.777068575514505\n",
            "iteration 590 accuracy 0.24036 loss 15.738901049004582\n",
            "iteration 591 accuracy 0.2404 loss 15.70104841734078\n",
            "iteration 592 accuracy 0.24056 loss 15.663433322261781\n",
            "iteration 593 accuracy 0.2407 loss 15.62609641374217\n",
            "iteration 594 accuracy 0.24092 loss 15.589013775294454\n",
            "iteration 595 accuracy 0.24118 loss 15.552155393340431\n",
            "iteration 596 accuracy 0.2415 loss 15.515225236419663\n",
            "iteration 597 accuracy 0.24234 loss 15.47834820510488\n",
            "iteration 598 accuracy 0.24268 loss 15.441955195468807\n",
            "iteration 599 accuracy 0.2428 loss 15.4057217020403\n",
            "iteration 600 accuracy 0.24288 loss 15.369286976437184\n",
            "iteration 601 accuracy 0.243 loss 15.333216901109331\n",
            "iteration 602 accuracy 0.2431 loss 15.297292360242034\n",
            "iteration 603 accuracy 0.24336 loss 15.261465096899542\n",
            "iteration 604 accuracy 0.24348 loss 15.225794113336963\n",
            "iteration 605 accuracy 0.24362 loss 15.188907855046695\n",
            "iteration 606 accuracy 0.24368 loss 15.1536331461593\n",
            "iteration 607 accuracy 0.24392 loss 15.118326031180445\n",
            "iteration 608 accuracy 0.24408 loss 15.08319847990681\n",
            "iteration 609 accuracy 0.24416 loss 15.048242616392033\n",
            "iteration 610 accuracy 0.2444 loss 15.013460600391571\n",
            "iteration 611 accuracy 0.24452 loss 14.978540278517311\n",
            "iteration 612 accuracy 0.24472 loss 14.943717125664223\n",
            "iteration 613 accuracy 0.24502 loss 14.908899472699176\n",
            "iteration 614 accuracy 0.24522 loss 14.874516749393479\n",
            "iteration 615 accuracy 0.24554 loss 14.840467611870116\n",
            "iteration 616 accuracy 0.2457 loss 14.806451912988765\n",
            "iteration 617 accuracy 0.2459 loss 14.772278777074401\n",
            "iteration 618 accuracy 0.2461 loss 14.738329347743486\n",
            "iteration 619 accuracy 0.24618 loss 14.704439451749035\n",
            "iteration 620 accuracy 0.24646 loss 14.670739232065152\n",
            "iteration 621 accuracy 0.24654 loss 14.637253436059714\n",
            "iteration 622 accuracy 0.24682 loss 14.603949799456272\n",
            "iteration 623 accuracy 0.24704 loss 14.570824190306727\n",
            "iteration 624 accuracy 0.24724 loss 14.537872009934535\n",
            "iteration 625 accuracy 0.24738 loss 14.505100068443884\n",
            "iteration 626 accuracy 0.24758 loss 14.472477805726664\n",
            "iteration 627 accuracy 0.24786 loss 14.439999482257404\n",
            "iteration 628 accuracy 0.24814 loss 14.407635917192096\n",
            "iteration 629 accuracy 0.24838 loss 14.375386510560913\n",
            "iteration 630 accuracy 0.24852 loss 14.343285715546859\n",
            "iteration 631 accuracy 0.2488 loss 14.31131934168407\n",
            "iteration 632 accuracy 0.24896 loss 14.279563228025506\n",
            "iteration 633 accuracy 0.24946 loss 14.248142163422823\n",
            "iteration 634 accuracy 0.2496 loss 14.21663659142719\n",
            "iteration 635 accuracy 0.24992 loss 14.18523043696998\n",
            "iteration 636 accuracy 0.24998 loss 14.153945240302196\n",
            "iteration 637 accuracy 0.25 loss 14.122826293899054\n",
            "iteration 638 accuracy 0.25008 loss 14.091870773207386\n",
            "iteration 639 accuracy 0.25034 loss 14.06102754490997\n",
            "iteration 640 accuracy 0.25056 loss 14.030328966358363\n",
            "iteration 641 accuracy 0.2508 loss 13.99977134330869\n",
            "iteration 642 accuracy 0.25098 loss 13.96933996406315\n",
            "iteration 643 accuracy 0.25122 loss 13.939033080850539\n",
            "iteration 644 accuracy 0.25136 loss 13.908876993185437\n",
            "iteration 645 accuracy 0.25164 loss 13.878585643722467\n",
            "iteration 646 accuracy 0.25186 loss 13.848575683645128\n",
            "iteration 647 accuracy 0.25202 loss 13.818839315905867\n",
            "iteration 648 accuracy 0.25212 loss 13.789240225336123\n",
            "iteration 649 accuracy 0.25256 loss 13.759777833356297\n",
            "iteration 650 accuracy 0.2528 loss 13.730437328266024\n",
            "iteration 651 accuracy 0.25294 loss 13.701252321495547\n",
            "iteration 652 accuracy 0.25304 loss 13.672123867401009\n",
            "iteration 653 accuracy 0.25324 loss 13.643028026795914\n",
            "iteration 654 accuracy 0.25368 loss 13.614252626854634\n",
            "iteration 655 accuracy 0.2538 loss 13.585431949537952\n",
            "iteration 656 accuracy 0.25396 loss 13.55660788519543\n",
            "iteration 657 accuracy 0.25414 loss 13.527902939321557\n",
            "iteration 658 accuracy 0.25428 loss 13.499350203104523\n",
            "iteration 659 accuracy 0.2545 loss 13.47091707399182\n",
            "iteration 660 accuracy 0.2547 loss 13.442622543484465\n",
            "iteration 661 accuracy 0.25482 loss 13.414476249987645\n",
            "iteration 662 accuracy 0.2549 loss 13.385883009862312\n",
            "iteration 663 accuracy 0.25508 loss 13.3579862604609\n",
            "iteration 664 accuracy 0.2553 loss 13.330248720914494\n",
            "iteration 665 accuracy 0.25564 loss 13.302393005290531\n",
            "iteration 666 accuracy 0.25584 loss 13.274537588586702\n",
            "iteration 667 accuracy 0.25582 loss 13.24715536713977\n",
            "iteration 668 accuracy 0.25618 loss 13.219954069047429\n",
            "iteration 669 accuracy 0.25626 loss 13.19280087666217\n",
            "iteration 670 accuracy 0.25652 loss 13.165756026815577\n",
            "iteration 671 accuracy 0.25676 loss 13.13877969618078\n",
            "iteration 672 accuracy 0.25704 loss 13.111858531488947\n",
            "iteration 673 accuracy 0.2573 loss 13.084957911003938\n",
            "iteration 674 accuracy 0.2575 loss 13.058160241403662\n",
            "iteration 675 accuracy 0.25768 loss 13.031486744825488\n",
            "iteration 676 accuracy 0.2579 loss 13.004902747426026\n",
            "iteration 677 accuracy 0.25806 loss 12.97842283048722\n",
            "iteration 678 accuracy 0.25828 loss 12.952288472520783\n",
            "iteration 679 accuracy 0.25844 loss 12.926040549578143\n",
            "iteration 680 accuracy 0.25872 loss 12.8999381852551\n",
            "iteration 681 accuracy 0.25904 loss 12.873951494422949\n",
            "iteration 682 accuracy 0.25922 loss 12.848080565688067\n",
            "iteration 683 accuracy 0.25944 loss 12.822310999121045\n",
            "iteration 684 accuracy 0.25956 loss 12.796652804642573\n",
            "iteration 685 accuracy 0.2597 loss 12.771050419224935\n",
            "iteration 686 accuracy 0.2598 loss 12.745574275857056\n",
            "iteration 687 accuracy 0.25994 loss 12.720175873448685\n",
            "iteration 688 accuracy 0.2601 loss 12.694845090340316\n",
            "iteration 689 accuracy 0.26028 loss 12.669633619929012\n",
            "iteration 690 accuracy 0.26054 loss 12.644543037393953\n",
            "iteration 691 accuracy 0.26084 loss 12.619540345252611\n",
            "iteration 692 accuracy 0.26098 loss 12.594644719460119\n",
            "iteration 693 accuracy 0.26116 loss 12.5698653086245\n",
            "iteration 694 accuracy 0.26126 loss 12.545168518255782\n",
            "iteration 695 accuracy 0.26144 loss 12.520558611952607\n",
            "iteration 696 accuracy 0.26172 loss 12.496039261070347\n",
            "iteration 697 accuracy 0.26182 loss 12.471614074305913\n",
            "iteration 698 accuracy 0.2619 loss 12.447272000919618\n",
            "iteration 699 accuracy 0.26204 loss 12.423047668150511\n",
            "iteration 700 accuracy 0.26214 loss 12.398912919404983\n",
            "iteration 701 accuracy 0.26216 loss 12.374891815016047\n",
            "iteration 702 accuracy 0.26224 loss 12.351002849648086\n",
            "iteration 703 accuracy 0.26244 loss 12.327187060651507\n",
            "iteration 704 accuracy 0.26262 loss 12.303455750059191\n",
            "iteration 705 accuracy 0.2629 loss 12.279776810865929\n",
            "iteration 706 accuracy 0.26302 loss 12.256179439204951\n",
            "iteration 707 accuracy 0.26314 loss 12.232662140765905\n",
            "iteration 708 accuracy 0.2632 loss 12.209241558533778\n",
            "iteration 709 accuracy 0.26346 loss 12.18589497672864\n",
            "iteration 710 accuracy 0.26362 loss 12.162589239060289\n",
            "iteration 711 accuracy 0.26382 loss 12.139359165170053\n",
            "iteration 712 accuracy 0.26404 loss 12.116227700594148\n",
            "iteration 713 accuracy 0.26414 loss 12.093174358990712\n",
            "iteration 714 accuracy 0.26438 loss 12.070181278896584\n",
            "iteration 715 accuracy 0.26446 loss 12.047258099256469\n",
            "iteration 716 accuracy 0.26468 loss 12.024435423072145\n",
            "iteration 717 accuracy 0.265 loss 12.001705025817799\n",
            "iteration 718 accuracy 0.26506 loss 11.979072967940466\n",
            "iteration 719 accuracy 0.26528 loss 11.956530649641936\n",
            "iteration 720 accuracy 0.26556 loss 11.934095556307245\n",
            "iteration 721 accuracy 0.26568 loss 11.91173764358695\n",
            "iteration 722 accuracy 0.26574 loss 11.889478332570944\n",
            "iteration 723 accuracy 0.26574 loss 11.867314288471572\n",
            "iteration 724 accuracy 0.26606 loss 11.845248307869447\n",
            "iteration 725 accuracy 0.26626 loss 11.823270851699963\n",
            "iteration 726 accuracy 0.26642 loss 11.801371432588404\n",
            "iteration 727 accuracy 0.26658 loss 11.779556716807045\n",
            "iteration 728 accuracy 0.26686 loss 11.757831963021728\n",
            "iteration 729 accuracy 0.2671 loss 11.736194520989962\n",
            "iteration 730 accuracy 0.26716 loss 11.714635074131822\n",
            "iteration 731 accuracy 0.26742 loss 11.693168858564073\n",
            "iteration 732 accuracy 0.26746 loss 11.67178522462272\n",
            "iteration 733 accuracy 0.26768 loss 11.650478167220847\n",
            "iteration 734 accuracy 0.26778 loss 11.629240527442567\n",
            "iteration 735 accuracy 0.26784 loss 11.608079657917836\n",
            "iteration 736 accuracy 0.26792 loss 11.58698541075529\n",
            "iteration 737 accuracy 0.26808 loss 11.565977239532007\n",
            "iteration 738 accuracy 0.26822 loss 11.545028739038342\n",
            "iteration 739 accuracy 0.26846 loss 11.524153650373716\n",
            "iteration 740 accuracy 0.26864 loss 11.503344055594637\n",
            "iteration 741 accuracy 0.26878 loss 11.482617704880916\n",
            "iteration 742 accuracy 0.26898 loss 11.461948603350793\n",
            "iteration 743 accuracy 0.2692 loss 11.441750181359808\n",
            "iteration 744 accuracy 0.26942 loss 11.421237405942248\n",
            "iteration 745 accuracy 0.26958 loss 11.400797473272622\n",
            "iteration 746 accuracy 0.26976 loss 11.380436210896157\n",
            "iteration 747 accuracy 0.26988 loss 11.360133826515273\n",
            "iteration 748 accuracy 0.27012 loss 11.339882014532389\n",
            "iteration 749 accuracy 0.27032 loss 11.319688375925866\n",
            "iteration 750 accuracy 0.27056 loss 11.2996503032675\n",
            "iteration 751 accuracy 0.27074 loss 11.279607918611163\n",
            "iteration 752 accuracy 0.27096 loss 11.259648655526423\n",
            "iteration 753 accuracy 0.2711 loss 11.239711979841637\n",
            "iteration 754 accuracy 0.27134 loss 11.219862246365079\n",
            "iteration 755 accuracy 0.27144 loss 11.20007493364429\n",
            "iteration 756 accuracy 0.27152 loss 11.180383708952748\n",
            "iteration 757 accuracy 0.2716 loss 11.160746944192544\n",
            "iteration 758 accuracy 0.2719 loss 11.141749698896243\n",
            "iteration 759 accuracy 0.2721 loss 11.122234030220397\n",
            "iteration 760 accuracy 0.27226 loss 11.102744972475778\n",
            "iteration 761 accuracy 0.27262 loss 11.08283313733265\n",
            "iteration 762 accuracy 0.27274 loss 11.063561943385192\n",
            "iteration 763 accuracy 0.27292 loss 11.044286103439136\n",
            "iteration 764 accuracy 0.27304 loss 11.025061913624123\n",
            "iteration 765 accuracy 0.27312 loss 11.005902333963444\n",
            "iteration 766 accuracy 0.27324 loss 10.98681308253312\n",
            "iteration 767 accuracy 0.27334 loss 10.967786811484176\n",
            "iteration 768 accuracy 0.2735 loss 10.948794921149972\n",
            "iteration 769 accuracy 0.27374 loss 10.929857231864258\n",
            "iteration 770 accuracy 0.27398 loss 10.91105989305664\n",
            "iteration 771 accuracy 0.27418 loss 10.892193418452933\n",
            "iteration 772 accuracy 0.27438 loss 10.873431975034693\n",
            "iteration 773 accuracy 0.27464 loss 10.85470674421816\n",
            "iteration 774 accuracy 0.27482 loss 10.836028776885366\n",
            "iteration 775 accuracy 0.275 loss 10.817381736691164\n",
            "iteration 776 accuracy 0.27512 loss 10.798808974995278\n",
            "iteration 777 accuracy 0.27534 loss 10.780303335934606\n",
            "iteration 778 accuracy 0.27546 loss 10.761822523514107\n",
            "iteration 779 accuracy 0.27564 loss 10.743382676521275\n",
            "iteration 780 accuracy 0.2758 loss 10.724995090760638\n",
            "iteration 781 accuracy 0.27592 loss 10.70668587204695\n",
            "iteration 782 accuracy 0.27618 loss 10.68852207321091\n",
            "iteration 783 accuracy 0.27634 loss 10.670347314334897\n",
            "iteration 784 accuracy 0.27654 loss 10.652263637344934\n",
            "iteration 785 accuracy 0.2767 loss 10.634240674106955\n",
            "iteration 786 accuracy 0.27676 loss 10.616297662790716\n",
            "iteration 787 accuracy 0.27698 loss 10.59841483701225\n",
            "iteration 788 accuracy 0.277 loss 10.580571473292226\n",
            "iteration 789 accuracy 0.2772 loss 10.562816724876487\n",
            "iteration 790 accuracy 0.27736 loss 10.545120973365641\n",
            "iteration 791 accuracy 0.27752 loss 10.527467046680378\n",
            "iteration 792 accuracy 0.2777 loss 10.509898602288382\n",
            "iteration 793 accuracy 0.27772 loss 10.49237601713125\n",
            "iteration 794 accuracy 0.27782 loss 10.474921235662073\n",
            "iteration 795 accuracy 0.27784 loss 10.457533522138577\n",
            "iteration 796 accuracy 0.278 loss 10.440195168977526\n",
            "iteration 797 accuracy 0.27812 loss 10.422936675184472\n",
            "iteration 798 accuracy 0.27822 loss 10.4057188130208\n",
            "iteration 799 accuracy 0.27838 loss 10.388554611212905\n",
            "iteration 800 accuracy 0.2786 loss 10.371465321849602\n",
            "iteration 801 accuracy 0.27884 loss 10.354425598144084\n",
            "iteration 802 accuracy 0.27888 loss 10.337439761178263\n",
            "iteration 803 accuracy 0.27904 loss 10.320511460119489\n",
            "iteration 804 accuracy 0.2791 loss 10.3036346507213\n",
            "iteration 805 accuracy 0.27928 loss 10.28682314623\n",
            "iteration 806 accuracy 0.27942 loss 10.270079130105108\n",
            "iteration 807 accuracy 0.27948 loss 10.253402192542747\n",
            "iteration 808 accuracy 0.2796 loss 10.236785339741864\n",
            "iteration 809 accuracy 0.27978 loss 10.220211680479384\n",
            "iteration 810 accuracy 0.27992 loss 10.203707355267404\n",
            "iteration 811 accuracy 0.2801 loss 10.187249866581665\n",
            "iteration 812 accuracy 0.28024 loss 10.170871798763656\n",
            "iteration 813 accuracy 0.28036 loss 10.154540727650708\n",
            "iteration 814 accuracy 0.28054 loss 10.13826317151187\n",
            "iteration 815 accuracy 0.28062 loss 10.122048302626425\n",
            "iteration 816 accuracy 0.28076 loss 10.105882491709462\n",
            "iteration 817 accuracy 0.2809 loss 10.089777803079825\n",
            "iteration 818 accuracy 0.2811 loss 10.073722035337713\n",
            "iteration 819 accuracy 0.28118 loss 10.057722532833067\n",
            "iteration 820 accuracy 0.2813 loss 10.041523116815798\n",
            "iteration 821 accuracy 0.2814 loss 10.025637704564318\n",
            "iteration 822 accuracy 0.2814 loss 10.009790474478482\n",
            "iteration 823 accuracy 0.28154 loss 9.994001048168984\n",
            "iteration 824 accuracy 0.28166 loss 9.978259043747503\n",
            "iteration 825 accuracy 0.28162 loss 9.96212103849113\n",
            "iteration 826 accuracy 0.28174 loss 9.946486321390648\n",
            "iteration 827 accuracy 0.28192 loss 9.930915764706329\n",
            "iteration 828 accuracy 0.28206 loss 9.915435092258646\n",
            "iteration 829 accuracy 0.2823 loss 9.899971789620599\n",
            "iteration 830 accuracy 0.28242 loss 9.884565640680608\n",
            "iteration 831 accuracy 0.28252 loss 9.869312587376156\n",
            "iteration 832 accuracy 0.28272 loss 9.85399462628442\n",
            "iteration 833 accuracy 0.28282 loss 9.838739212568868\n",
            "iteration 834 accuracy 0.28292 loss 9.823532460448218\n",
            "iteration 835 accuracy 0.28314 loss 9.808375646532824\n",
            "iteration 836 accuracy 0.2834 loss 9.79327909774749\n",
            "iteration 837 accuracy 0.28344 loss 9.778230751273599\n",
            "iteration 838 accuracy 0.28346 loss 9.76322986007143\n",
            "iteration 839 accuracy 0.2836 loss 9.748271348554676\n",
            "iteration 840 accuracy 0.28372 loss 9.73337035013861\n",
            "iteration 841 accuracy 0.28386 loss 9.718505171458332\n",
            "iteration 842 accuracy 0.28398 loss 9.703686676259501\n",
            "iteration 843 accuracy 0.2842 loss 9.688896834723323\n",
            "iteration 844 accuracy 0.28468 loss 9.674239050230858\n",
            "iteration 845 accuracy 0.285 loss 9.659543216103891\n",
            "iteration 846 accuracy 0.28514 loss 9.644884837190098\n",
            "iteration 847 accuracy 0.28518 loss 9.630276826885598\n",
            "iteration 848 accuracy 0.28538 loss 9.61570292626056\n",
            "iteration 849 accuracy 0.2856 loss 9.601194987710455\n",
            "iteration 850 accuracy 0.28566 loss 9.586728842872219\n",
            "iteration 851 accuracy 0.28584 loss 9.572327241745642\n",
            "iteration 852 accuracy 0.28606 loss 9.557952743013768\n",
            "iteration 853 accuracy 0.28622 loss 9.543622268397005\n",
            "iteration 854 accuracy 0.28636 loss 9.529331168090971\n",
            "iteration 855 accuracy 0.28644 loss 9.51507242813463\n",
            "iteration 856 accuracy 0.28662 loss 9.500842541807456\n",
            "iteration 857 accuracy 0.28678 loss 9.48665350911427\n",
            "iteration 858 accuracy 0.2869 loss 9.472515717013616\n",
            "iteration 859 accuracy 0.28704 loss 9.458375536771822\n",
            "iteration 860 accuracy 0.28726 loss 9.444328074655992\n",
            "iteration 861 accuracy 0.28736 loss 9.43031901041377\n",
            "iteration 862 accuracy 0.28744 loss 9.41634214925785\n",
            "iteration 863 accuracy 0.28758 loss 9.402425854686868\n",
            "iteration 864 accuracy 0.28768 loss 9.388545369780667\n",
            "iteration 865 accuracy 0.28766 loss 9.374703337261039\n",
            "iteration 866 accuracy 0.2878 loss 9.360904755430699\n",
            "iteration 867 accuracy 0.28796 loss 9.347157699094186\n",
            "iteration 868 accuracy 0.2882 loss 9.333436846487604\n",
            "iteration 869 accuracy 0.28842 loss 9.319760201728183\n",
            "iteration 870 accuracy 0.2885 loss 9.306115481052961\n",
            "iteration 871 accuracy 0.28862 loss 9.29232446833552\n",
            "iteration 872 accuracy 0.28874 loss 9.2787739253375\n",
            "iteration 873 accuracy 0.28888 loss 9.265273409542795\n",
            "iteration 874 accuracy 0.28908 loss 9.251679944892937\n",
            "iteration 875 accuracy 0.28906 loss 9.238259726777546\n",
            "iteration 876 accuracy 0.28918 loss 9.224872233179802\n",
            "iteration 877 accuracy 0.28944 loss 9.211531037856957\n",
            "iteration 878 accuracy 0.2896 loss 9.198232339296565\n",
            "iteration 879 accuracy 0.28978 loss 9.184982365271114\n",
            "iteration 880 accuracy 0.29004 loss 9.171725523699447\n",
            "iteration 881 accuracy 0.29018 loss 9.158532856210972\n",
            "iteration 882 accuracy 0.29036 loss 9.145375089917433\n",
            "iteration 883 accuracy 0.29048 loss 9.132253478010016\n",
            "iteration 884 accuracy 0.29064 loss 9.119173796992719\n",
            "iteration 885 accuracy 0.29078 loss 9.106139573872545\n",
            "iteration 886 accuracy 0.29086 loss 9.09312520315169\n",
            "iteration 887 accuracy 0.291 loss 9.079961900493483\n",
            "iteration 888 accuracy 0.2912 loss 9.06682548104789\n",
            "iteration 889 accuracy 0.2912 loss 9.052718949580886\n",
            "iteration 890 accuracy 0.2913 loss 9.039678305740399\n",
            "iteration 891 accuracy 0.29148 loss 9.02666518783094\n",
            "iteration 892 accuracy 0.29164 loss 9.01366329793498\n",
            "iteration 893 accuracy 0.29182 loss 9.000699172185364\n",
            "iteration 894 accuracy 0.29194 loss 8.987738387573208\n",
            "iteration 895 accuracy 0.29206 loss 8.974842186247361\n",
            "iteration 896 accuracy 0.29228 loss 8.961992396980916\n",
            "iteration 897 accuracy 0.29246 loss 8.94917635110105\n",
            "iteration 898 accuracy 0.29272 loss 8.936406802270385\n",
            "iteration 899 accuracy 0.29284 loss 8.923676249953182\n",
            "iteration 900 accuracy 0.29292 loss 8.910982396758618\n",
            "iteration 901 accuracy 0.29304 loss 8.898336339221146\n",
            "iteration 902 accuracy 0.29322 loss 8.885725278488938\n",
            "iteration 903 accuracy 0.29336 loss 8.873113602591125\n",
            "iteration 904 accuracy 0.29352 loss 8.86055400706061\n",
            "iteration 905 accuracy 0.29366 loss 8.848031140018058\n",
            "iteration 906 accuracy 0.29372 loss 8.835540651448706\n",
            "iteration 907 accuracy 0.29384 loss 8.823093157614448\n",
            "iteration 908 accuracy 0.2939 loss 8.810684786588668\n",
            "iteration 909 accuracy 0.29402 loss 8.798307012057718\n",
            "iteration 910 accuracy 0.2942 loss 8.785971699789128\n",
            "iteration 911 accuracy 0.29416 loss 8.773920970467918\n",
            "iteration 912 accuracy 0.29434 loss 8.761660389701548\n",
            "iteration 913 accuracy 0.29446 loss 8.749433930208884\n",
            "iteration 914 accuracy 0.29446 loss 8.737068592250601\n",
            "iteration 915 accuracy 0.29478 loss 8.724912866282772\n",
            "iteration 916 accuracy 0.29498 loss 8.71278408172625\n",
            "iteration 917 accuracy 0.29516 loss 8.700699279610406\n",
            "iteration 918 accuracy 0.29528 loss 8.688657082850941\n",
            "iteration 919 accuracy 0.29544 loss 8.67664020076386\n",
            "iteration 920 accuracy 0.2955 loss 8.664676416883557\n",
            "iteration 921 accuracy 0.29564 loss 8.652738206369039\n",
            "iteration 922 accuracy 0.2957 loss 8.640824393252709\n",
            "iteration 923 accuracy 0.29576 loss 8.628954048094009\n",
            "iteration 924 accuracy 0.29574 loss 8.617118313571542\n",
            "iteration 925 accuracy 0.29594 loss 8.605319259484823\n",
            "iteration 926 accuracy 0.2962 loss 8.59354817206414\n",
            "iteration 927 accuracy 0.29628 loss 8.581820559778619\n",
            "iteration 928 accuracy 0.2964 loss 8.570125904421147\n",
            "iteration 929 accuracy 0.29652 loss 8.55845628232552\n",
            "iteration 930 accuracy 0.29668 loss 8.546833854694286\n",
            "iteration 931 accuracy 0.29684 loss 8.535244316737696\n",
            "iteration 932 accuracy 0.29692 loss 8.523682938659178\n",
            "iteration 933 accuracy 0.29704 loss 8.51213108844343\n",
            "iteration 934 accuracy 0.29718 loss 8.500598804750046\n",
            "iteration 935 accuracy 0.29728 loss 8.489108449424187\n",
            "iteration 936 accuracy 0.29742 loss 8.477657984478066\n",
            "iteration 937 accuracy 0.29756 loss 8.466241908806488\n",
            "iteration 938 accuracy 0.2976 loss 8.454853106701743\n",
            "iteration 939 accuracy 0.29772 loss 8.443503147444263\n",
            "iteration 940 accuracy 0.29804 loss 8.431692311903367\n",
            "iteration 941 accuracy 0.2982 loss 8.420402873286486\n",
            "iteration 942 accuracy 0.29826 loss 8.409158417828925\n",
            "iteration 943 accuracy 0.2984 loss 8.397937993163922\n",
            "iteration 944 accuracy 0.29846 loss 8.386740452719703\n",
            "iteration 945 accuracy 0.29866 loss 8.375572838235346\n",
            "iteration 946 accuracy 0.29872 loss 8.364431885628278\n",
            "iteration 947 accuracy 0.29886 loss 8.353283052448145\n",
            "iteration 948 accuracy 0.29898 loss 8.342237314601059\n",
            "iteration 949 accuracy 0.29914 loss 8.331156492755241\n",
            "iteration 950 accuracy 0.29922 loss 8.32008589983926\n",
            "iteration 951 accuracy 0.29936 loss 8.30902857127596\n",
            "iteration 952 accuracy 0.2995 loss 8.297973269147244\n",
            "iteration 953 accuracy 0.2996 loss 8.286958458684065\n",
            "iteration 954 accuracy 0.29992 loss 8.275982673304323\n",
            "iteration 955 accuracy 0.3 loss 8.265015913350997\n",
            "iteration 956 accuracy 0.30006 loss 8.254074290627337\n",
            "iteration 957 accuracy 0.30014 loss 8.243169774684356\n",
            "iteration 958 accuracy 0.30024 loss 8.232301935406564\n",
            "iteration 959 accuracy 0.30032 loss 8.221461257693258\n",
            "iteration 960 accuracy 0.30026 loss 8.210650769897654\n",
            "iteration 961 accuracy 0.30054 loss 8.199861461859179\n",
            "iteration 962 accuracy 0.30066 loss 8.189098504030508\n",
            "iteration 963 accuracy 0.30068 loss 8.178372099752778\n",
            "iteration 964 accuracy 0.30076 loss 8.167670663519763\n",
            "iteration 965 accuracy 0.30084 loss 8.156996049020856\n",
            "iteration 966 accuracy 0.30092 loss 8.146357733319899\n",
            "iteration 967 accuracy 0.30104 loss 8.135738073611794\n",
            "iteration 968 accuracy 0.30116 loss 8.125127245362219\n",
            "iteration 969 accuracy 0.30118 loss 8.114548220574155\n",
            "iteration 970 accuracy 0.30124 loss 8.103985250077288\n",
            "iteration 971 accuracy 0.30134 loss 8.093439325905475\n",
            "iteration 972 accuracy 0.30136 loss 8.082940412796589\n",
            "iteration 973 accuracy 0.30146 loss 8.07246564925324\n",
            "iteration 974 accuracy 0.30154 loss 8.062023456992177\n",
            "iteration 975 accuracy 0.30174 loss 8.051612678536102\n",
            "iteration 976 accuracy 0.302 loss 8.041292673449274\n",
            "iteration 977 accuracy 0.30212 loss 8.0309357265808\n",
            "iteration 978 accuracy 0.30216 loss 8.020603833499225\n",
            "iteration 979 accuracy 0.30228 loss 8.010298588934264\n",
            "iteration 980 accuracy 0.30242 loss 8.000028073828464\n",
            "iteration 981 accuracy 0.3025 loss 7.989779892080739\n",
            "iteration 982 accuracy 0.30254 loss 7.979558203735765\n",
            "iteration 983 accuracy 0.30264 loss 7.969371356078511\n",
            "iteration 984 accuracy 0.30268 loss 7.959211517994469\n",
            "iteration 985 accuracy 0.30278 loss 7.949076384366768\n",
            "iteration 986 accuracy 0.30284 loss 7.938976730804854\n",
            "iteration 987 accuracy 0.30306 loss 7.928904053108703\n",
            "iteration 988 accuracy 0.30308 loss 7.918855593042052\n",
            "iteration 989 accuracy 0.30316 loss 7.908841364127328\n",
            "iteration 990 accuracy 0.30318 loss 7.898886990162384\n",
            "iteration 991 accuracy 0.30338 loss 7.888924074568631\n",
            "iteration 992 accuracy 0.30352 loss 7.878991660362641\n",
            "iteration 993 accuracy 0.30364 loss 7.869092332766447\n",
            "iteration 994 accuracy 0.30366 loss 7.859212847046246\n",
            "iteration 995 accuracy 0.30376 loss 7.849364833160138\n",
            "iteration 996 accuracy 0.30374 loss 7.839535794209477\n",
            "iteration 997 accuracy 0.30386 loss 7.82973821743868\n",
            "iteration 998 accuracy 0.30388 loss 7.8199686126221435\n",
            "iteration 999 accuracy 0.304 loss 7.810220394581031\n"
          ]
        }
      ],
      "source": [
        "layers = [\n",
        "    Layer(35, 'leakyRelu'),\n",
        "    Layer(20, 'leakyRelu'),\n",
        "    OutputLayer(10, 'leakyRelu')\n",
        "]\n",
        "nn = NeuralNetwork(layers, 18)\n",
        "nn.train(X_train, y_train, 1e-7, 1000)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3cUY-qa7-CvS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Sup2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}