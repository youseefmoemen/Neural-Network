{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sup2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GON6iRSbG-PX"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "  return 1 / (1 + np.exp(-z))\n",
        "def sigmoid_grad(z):\n",
        "  t = sigmoid(z)\n",
        "  return t * (1 - t)"
      ],
      "metadata": {
        "id": "tv84SdZlPrxd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Activations = {\n",
        "    'sigmoid': sigmoid\n",
        "}\n",
        "Activations_grad = {\n",
        "    'sigmoid': sigmoid_grad\n",
        "}"
      ],
      "metadata": {
        "id": "A8eh2OJgP7IB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer():\n",
        "  def __init__(self, n_neurons, activation):\n",
        "    self.n_neurons = n_neurons\n",
        "    self.w = None\n",
        "    self.activation = Activations[activation]\n",
        "    self.activation_grad = Activations_grad[activation]\n",
        "  def initialize(self, prev):\n",
        "    self.w = np.random.normal(size=(prev, self.n_neurons))\n",
        "  def compute(self, Oi):\n",
        "    Netj = np.dot(Oi, self.w)\n",
        "    Oj = self.activation(Netj)\n",
        "    return Netj, Oj\n",
        "  def grad(self, Dk, w_next, Oi):\n",
        "    partial = np.dot(Dk, w_next.T)\n",
        "    Netj = self.compute(Oi)[0]\n",
        "    delta = np.multiply(partial, self.activation_grad(Netj))\n",
        "    Dw = np.dot(Oi.T, delta)\n",
        "    return Dw, delta\n",
        "  def update(self, learning_rate, Dw):\n",
        "    self.w = self.w - learning_rate * Dw"
      ],
      "metadata": {
        "id": "JDuzFFNDBMWj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class OutputLayer(Layer):\n",
        "  def __init__(self, *args, **kwargs):\n",
        "    super().__init__(*args, **kwargs)\n",
        "  def grad(self, target, Oi):\n",
        "    Netj, Oj = self.compute(Oi)\n",
        "    delta = np.multiply(target - Oj, self.activation_grad(Netj))\n",
        "    Dw = np.dot(Oi.T, delta)\n",
        "    return Dw, delta"
      ],
      "metadata": {
        "id": "OMW5bsyJUuDX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork():\n",
        "  def __init__(self, layers, input_shape):\n",
        "    self.layers = layers\n",
        "    self.O_ALL = None\n",
        "    self.input_shape = input_shape\n",
        "    self.layers[0].initialize(self.input_shape)\n",
        "    for i in range(1, len(self.layers)):\n",
        "      self.layers[i].initialize(self.layers[i-1].n_neurons)\n",
        "  def predict(self, X):\n",
        "    Oi = X\n",
        "    self.O_ALL = []\n",
        "    self.O_ALL.append(Oi)\n",
        "    for layer in self.layers:\n",
        "      Oi = layer.compute(Oi)[1]\n",
        "      self.O_ALL.append(Oi)\n",
        "    return Oi\n",
        "  def loss(self, Oj, y):\n",
        "    E = 0.5 * (np.sum(y - Oj) ** 2)\n",
        "    return E\n",
        "  #def updates(self):\n",
        "  #def compute_grades(self):\n",
        "  def train(self, X_train, y_train, learning_rate, num_iterations):\n",
        "    history = []\n",
        "    for i in range(num_iterations):\n",
        "      Oj = self.predict(X_train)\n",
        "      E = self.loss(Oj, y_train)\n",
        "      history.append((i ,E))\n",
        "      print(f'iteration: {i}    Loss = {E}')\n",
        "      for j in reversed(range(len(self.layers))):\n",
        "        if j == len(self.layers) - 1:\n",
        "          Dw, delta = self.layers[j].grad(y_train, self.O_ALL[j])\n",
        "        else:\n",
        "          Dw, delta = self.layers[j].grad(delta, self.layers[j+1].w, self.O_ALL[j])\n",
        "        self.layers[j].update(learning_rate, Dw)\n"
      ],
      "metadata": {
        "id": "-v5Lkksr4xb5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X1 = np.linspace(-1, 1, 100)\n",
        "X2 = (3 * X1 + 5) + np.random.normal(size=(100))\n",
        "X = np.c_[X1, X2]\n",
        "y = (X1 >= 0)"
      ],
      "metadata": {
        "id": "BVDBkkCgYS_D"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(X1[X1>=0], X2[X1>=0], color = 'red')\n",
        "plt.scatter(X1[X1<0], X2[X1<0])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "U65tymDc1XLh",
        "outputId": "adeaa6bb-94a7-4b07-acb9-07393eea213b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAD4CAYAAADFAawfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAacklEQVR4nO3df5Be1VkH8O+TTYBd25IQdhAWdjfMYLAjlsBOxWbGlsA0lXYgVtR0EqW1Tmx1tNNadJk4U8eZDFEcK46tdQfbUpOhaEoRJ1SEBnTMFHRTaANEIECXsqQlpYTRZsWQPP5x74W7b+5533vf95x7zzn3+5nJ7O59f5297+a5533Oc84RVQUREflrSdMNICKi7hioiYg8x0BNROQ5BmoiIs8xUBMReW6piyc988wzdXJy0sVTExFFad++fT9Q1dGi25wE6snJSczOzrp4aiKiKInInOk2pj6IiDzHQE1E5DkGaiIizzFQExF5joGaiMhzDNREVL+dO4HJSWDJkuTrzp1Nt8hrTsrziIiMdu4EtmwBjh5Nfp6bS34GgE2bmmuXx9ijJqJ6bd36RpDOHD2aHKdCDNREVK/nnqt2nBioiahm4+PVjhMDNRHVbNs2YGRk8bGRkeQ4FWKgJqJ6bdoEzMwAExOASPJ1ZoYDiV2w6oOI6rdpEwNzBexRExF5joGaiMrhJJXGMFATUW/ZJJW5OUD1jUkqRcGaAd06Bmoi6q3sJJUqAZ1KY6Amot7KTlJpctZhkz15x6/Nqg8i6m18POkdFx3Pa2rWYZPrh9Tw2uxRE1FvZSepNDXrsMmefA2vzUBNRL2VnaTS1KxDlz35XmmNGj5FMFATUTmbNgHf+Q5w4kTytehjfVOzDl315MsMjtbwKYKBmojsKhPQbXPVky+T1qjhUwQDNRGFz1VPvkxao4ZPEaKq1p4sMzU1pbOzs9afl4ioVpOTxdUuExPJpwWLRGSfqk4V3cYeNRGRSbe0Ro112wzUREQmprQGUOsMTKY+iIiqcpASYeqDiMimmmdgMlATEQHVcs41z8BkoCaievi8/GnVVf9qnoFZKlCLyMdF5DEReVREbhOR05y0hoji5Pvyp1XX66h5BmbPQC0iYwB+F8CUqv4UgCEAG520hojiVGU96yZ63f3knGucgVk29bEUwLCILAUwAuAFZy0ioviUCYQue929LgBNrfpXUs9ArarzAP4MwHMADgF4RVX/pfN+IrJFRGZFZPbw4cP2W0pE4SoTCF0tF1rmAtDUqn8llUl9rABwDYBVAM4B8GMisrnzfqo6o6pTqjo1Ojpqv6VEFK4ygdBVyVuZC0DZnHNDqZkyqY8rATyrqodV9RiAOwC8w22ziCgqZQKhq/RD2QtAr5xzgwOiZQL1cwAuE5EREREAVwA44LZZRBQdUyDMeqlzc0kQz7ORfrB1AWhwF5kyOeqHAOwC8E0A+9PHzDhuFxG1Qb6XCiQ91SxY2yp5s5V/bmo/SJTc3FZVPwXgU47bQkRtU9RLVbW7jGgW6LduTYLq+HgSpKteAMpu8OsAZyYSkZnrwbN+eqn9tMlGzXODlSEM1ERUrI7Bs6r54yZnODa1HyS4zCkRmdSxu0kWePPpj5ERcwCscceVunGZUyKqro7Bs6q91AYH9JrEQE3Ujc8rvrlW17TqKvljz6d6u8JATWTi+4pvrvk4rdrHNtWAgZrIpMEJDl5ocPBs4DZF9kmIg4lEJkuWJD3pTiLJx3TyU9UBSk9wMJGoHy3NhwYvwk9CDNREJi3Nh3qj3/RFhJUhDNREJj7maH1lOyc8yEBuhJ+EGKipvcoElxq3WwqWi+qYQdIXEX4SYqCmdmp76Z1NLnLCg6QvIvwkxKoPaqeIpyLXzkV1TAvfH1Z9EHWKcMCpMS5ywhGmLwbBQE3t1OSAU2STMZwE1QjTF4NgoKZ2shlcqgTeGHPjroIqB3Jfxxw1tdfOnYPv+sFlOsmSbjlqBmqiQVQNvJyWTgYcTCRypeqgpO3ceGz5birEQE00iKqB13ZuPLZ8NxVioCYaRNXAa3PgLcLFh2oXyCcS5qiJBmVjULIfzHcPpttAMFD7e8rBROrLnQ/P46Z7nsALRxZwzvJhXL9+NTasGWu6WZRhBclgTOdv5UpgYaH29aw5mEiV3fnwPG64Yz/mjyxAAcwfWcANd+zHnQ/PN900ynD23mBMA74vveRdSomBmgrddM8TWDh2fNGxhWPHcdM9TzTUIjoJZ+8NpmqlTYPLCzBQU6EXjixUOk4NcTF7L5ABtoGZPpGsXFl8/wbXs2agpkLnLB+udJwi0aaSP9Mnkptv9i6lxEBNha5fvxrDy4YWHRteNoTr169uqEVUi7aV/BV9IvEwpcSqDzJi1UcLseSvMd2qPpbW3RgKx4Y1YwzMbTM+XlyyFvB+gzFg6oMoNC4H+1jy5yUGaqKQuB7s8zA/SwzURGHIetGbN1cf7KvaA+eC/d5hjpqiFsWAaNGaFJ1MkzE6H5v1wAEG4ICwR03RimYafFHJXCfTYJ+p3G7z5rgns0SGgZqiFc00+F5Tl7sN9nV7bMyTWSLDQE3RimYafLfSuF6Dfb3K6mKezBKRUoFaRJaLyC4R+S8ROSAiP+u6YRS+Ox+ex9rte7BqejfWbt9Te8ohmmnwppK5HTt6D/YVPbbT3BzTIJ4r26O+GcA/q+qFAN4G4IC7JlEMfMgPRzMNfpCSufxju2EaxGs9p5CLyOkAHgFwvpacb84p5LR2+x7MF6QYxpYPY+/0utraEUXVhy1lqke46UBjBp1CvgrAYQBfEJG3AdgH4GOq+qOOF9kCYAsAjHO6aev5kh8Oehq87S2+ssdu3Vo8TRxodM1lMiuT+lgK4BIAf62qawD8CMB0551UdUZVp1R1anR01HIzKTTR5Ieb4moGYjaZxZQKYSfLS2UC9fMAnlfVh9KfdyEJ3ERG0eSHm+J6uVGu6RGUnoFaVb8H4Lsikv0PuwLA405bRcHbsGYMN77/IowtH4YgyU3f+P6Lwk1DlGVrwSRTCsJWaoJregSl1HrUInIxgFsAnALgGQAfUtWXTffnYCK1UtFgXb+7V3OH8dYZeBdyVX0kzT//tKpu6BakiVrLZrqCqQnK4cxEIltspiuYmlisLRvuGjBQEwF2AoGpYqLfSgouN5po04a7BgzU1C5FAdlWIGC6wo22bbhbgJvbUnuYBvuGh4GXXjr5/v0M3NmepEKt2XB34MFEoiiYemZFQRroP7fMdMUbfEwpBYiBmtqjauCtOxDENmDGlJI1DNTUHqbAu3Jl84EgxgEzW7nlbhUwsV3cTFTV+r9LL71UibyzY4fqyIhqEgqTfyMjyfEdO1QnJlRFkq87dtTbtomJxe3K/k1M1NuOKnqdM5Hi30nE3uub3s8AAZhVQ0xloKZ2aTogm7gOaraVCZKuLz4hXty66BaoWfVB3mvFmtKhTRkv016bU+qLRFYNwqoPCpYPO8XUIrQBszKzMF3PrmxRNQgDNXktmp3Ee2l6ynjVQbmyQdJluWK/F7cAByAZqMlrvuwUU4umarD7qTjx4RNAPxe3QKtrGKjJa9HvFOND766fMrqmPwHk21Hl4hbodHQOJpLXshx1Pv0xvGwojk0IXA+2lRXZoFxXHv+uHEykYEW9U4wvvbsWDcqF+ruW2YWcqFFB7yTejevttsratq24Z+9rxckgAv1d2aOuwZ0Pz2Pt9j1YNb0ba7fvia+0rI2q5paL7u9L786XfHMdAv1dmaN2LOoca1tVzS2b7n/ddcCttzafoyYvMEfdoNbUAbdJ1dyy6f533+2ud+dDNQlZwxy1Y2XrgFsxTToWVXPL3Y5v2mS/99zZg89qhQH21APFHrVjZeqAWzNNOhZVc8t156J9qSYhaxioHbt+/WoMLxtadGx42RCuX7/69Z+ZHglM1Vl5dc/i86WahKxhoHasTB1wyNOkW1nRUrVyoO5KA1+qScgaVn14YO32PZgvCMpjy4exd3pdAy0qhxUtnvJlxiNVwqoPz5VJj/iIKRtPBVorTGas+vBA1vt0XfVhu7Ik5JRN9FxUk1BjGKg94XqadGeaIqssyV67H+csHy5M2TS9sh1LHSk2TH20hIs0hY8pG5Y6UowYqFvCRZqi6ZXtiipOrFyQTLP6ONuPGsLUR0u4SlPUvbJdFoznjyxAAGQ1S1nPuTNIZ0pfkEyz+vbuXbwuB2f7UY3Yo+4Qa12wj2mKqvJpDeCNIJ1ZOHYcQyKFjy19QTLN6puZqT7bjz1wsoQ96hwXA26+KFtZ4mogzsbzFqU1Oh1XxfCyoZNqu0tfkEyz944bXtd0f663QRZxwktOqBNPADuB0NUEFlvPu2p690m96E5j6e/e97mYnEyCaqehoeJgPTGR7NVX9nlM96fW6zbhhT3qnFDrgm19Eug2EDdIoO41wFc2qJry7Jms5zxQ3ty0A4hp7WjTeh1cb4MsYo46J9Qdr22V3rm6UJken11QypbSFeXZs4y0tYoT06y+z3622my/JtbbYE48WuxR51y/fnXhR3TfB9xsBVhXlSGm5x0SqdSDr2sGp3FWX5XZfnXvzceceNQYqHNqCwSW2QqwZS9UVfPhpuftp5QumI1us+C4dWuS7hgfT4K0q6DZbQ1qBurglR5MFJEhALMA5lX1fd3uG+pgYqhsDgL2CsL9vlbR82b10J1CGLz1zpIlQNH/ZRHgxIn620OV2RpM/BiAAwDeYqVVA+J6Dm+w+UmgV4+13wFH0/OGmGry0vh4cZUJ16COQqlALSLnAngvgG0APuG0RSXEXO/cr7pSAjYHHENNNXmp7pw41apsj/ovAPw+gDeb7iAiWwBsAYBxx1dxV2Vk1JvtAcdgcs6+qzsnTrXqWZ4nIu8D8KKq7ut2P1WdUdUpVZ0aHR211sAiodY7x8DXqeixTv2vZNOmZDLNiRPJVwbpaJTpUa8FcLWIXAXgNABvEZEdqrrZbdPMfF0HuQ1spCtsjy8wFUaxqzSFXETeBeCTTVd9cK++cLl470Ke+k+UiW7PxKbXQab+udjAgKkwil2lCS+q+gCAB5y0pCIOQoXJRVBlKoxiF2SPmsLlYj0VXwc4iWxhoC6JVQV2uAiqTIVR7LjWRwmsKiivV0WHqw0MmAqjmHHjgBLKVhX4Mq29qXbYquhgVQ+1UXRVH3UrMwCW38+vzNrKrjTZDlsVHS4qQ4hCxkBdQpkBMF+CS5PtsFXRwXI7osUYqLvIBhDnjyygc2/rzgEwX4JLt91UXA+C2qroCHWnHSJXGKgN8ikEAFC8se3T8uFlOG3ZEnz89kdeD342g8sgFSbdXs91GsRWRQfL7YgWY6A2KEohKJIg/eprJ/Dy0WOLcsCXXzhqJbgMmmMuCnJ5LtMgtsrkWG5HtBirPgxWTe9GlTMzltu1ZJBqCxvrVmRVH6YduwXAs9vfW6ldROSWrR1eWsU0LdnkhSMLVmp5beS6s3aYgn6/6RgfSg+J2oipDwNTnnTFyLLC+9sa6DI9zxKRyjlrW7leX0oPidqKPWp07y12Hgfc7vNXtGM3ABxPU1RVZkXa2uqKO+oQNav1gbrX9HBTIHKVBugMrktEXg/SmSpB0pd0DBH1r/WBup/eout1JfLPv2p6d+F96gySXEaUqFmtz1H73lv0YfIH65qJmhVdj7pqdYLvvcWinHUWJOuqxLCV6yai/kQVqPtZjrRbIPRB2UFN10uvchlRouZENeGl38kiIdYIc0NXori0ZsJLrwWJTAE4xN7ioLn1/MXp9OFlEAGOHD0WzIWKqE2iGkxsckGiug0yyNg5geXIwrGT1i6J5TwRxSCqQN3kgkR1G6QSo6gkMS+m80QUg6BSH1X24zOt0+FL2d2gBqnEKHMOYjlPRDEIJlCXrehwsSCRr/rNrZdZcCqm80QUumBSH1W3mOIkDbNeKSKeJyK/BNOjrlrlwEkaZp3nhlUfRH4LJlD3M4MwxLK7uvDcEIXDy0BdNGjo+wxCIiJXvMtRmxapB8B99IiolbzrUXcbNNw7vY6BmYhax7sete/LjhIR1c27QG0aHFSg0n6BRESx8C5Qd6vx5ToUSQ5/7fY9lTe6JaJweReoN6wZe33QsEib16HgbuBE7eRdoAaSYL13eh3EcHtb89VVZ2cSURy8q/rI832brH4MskkBB1qJ2snLHnUmtvU6Bk1d+LDRLRHVz+tAnc9XxzDJZdDURWwXrk4cKCUq1jP1ISLnAfgSgLOQVMnNqOrNrhuWiWlNikFTFzEvNNXPxsREbVEmR/0agN9T1W+KyJsB7BORe1X1ccdti46NnHtMF668bp82Yvx9iaroGahV9RCAQ+n3/y0iBwCMAWCghnlwkAtLVcOBUiKzSjlqEZkEsAbAQy4a45KL/KdpcPAP79zPhaUq4kApkZmoark7irwJwL8C2KaqdxTcvgXAFgAYHx+/dG5uzmY7B9KZ/wSSnuygQdK03deQCI4XnNex5cPYO72u79eLmav3iCgUIrJPVaeKbivVoxaRZQC+AmBnUZAGAFWdUdUpVZ0aHR3tv7UOuJooYvpYXhSku92f4qvwIbKpTNWHAPhbAAdU9c/dN8k+V/lP0+CgqUfNj/HdxTpQSjSoMj3qtQB+FcA6EXkk/XeV43ZZ5Sr/aapr/sDPnBd1vTMR1atM1ce/A8ZlN4LgqtqiW13z1MQZUdY7E1H9Sg8mVjE1NaWzs7PWn9ekzPoZg6yxQUTkWrfBRK8XZSqj7Iw2H/OfvHgQURler/VRRqhLf3JtaSIqK/hAHeqMtlAvMERUv+ADdagz2kK9wBBR/YIP1KEu/RnqBYaI6hd8oA51RluoFxgiql/wVR+AnxUdvcS8tjQR2RVFoA5ViBcYIqpf8KkPIqLYMVATEXnOm9QHZ+kRERXzIlBzY1MiIjMvUh+cpUdEZOZFoOYsPSIiMy8CNWfpERGZeRGoOUuPiMjMi8FEztIjIjLzIlADnKVHRGTiReqDiIjMGKiJiDzHQE1E5DlvctR14nR1IgpJ6wI1p6sTUWhal/rgdHUiCk3rAjWnqxNRaFoXqDldnYhC07pAzenqRBSa1g0mcro6EYWmdYEa4HR1IgpL61IfREShYaAmIvIcAzURkecYqImIPMdATUTkOVFV+08qchjAXJ8PPxPADyw2xxa2qxq2qxq2q5oY2zWhqqNFNzgJ1IMQkVlVnWq6HZ3YrmrYrmrYrmra1i6mPoiIPMdATUTkOR8D9UzTDTBgu6phu6phu6ppVbu8y1ETEdFiPvaoiYgoh4GaiMhzjQRqEfklEXlMRE6IiLGURUTeIyJPiMhBEZnOHV8lIg+lx28XkVMstesMEblXRJ5Kv64ouM/lIvJI7t//isiG9LYvisizudsurqtd6f2O5177rtzxJs/XxSLyjfT9/raI/EruNqvny/T3krv91PT3P5iej8ncbTekx58QkfWDtKOPdn1CRB5Pz8/XRWQid1vhe1pTuz4oIodzr/8buduuS9/3p0Tkuprb9elcm54UkSO525ycLxH5vIi8KCKPGm4XEfnLtM3fFpFLcrcNfq5UtfZ/AH4SwGoADwCYMtxnCMDTAM4HcAqAbwF4a3rb3wPYmH7/OQAftdSuPwUwnX4/DeBPetz/DAA/BDCS/vxFANc6OF+l2gXgfwzHGztfAH4CwAXp9+cAOARgue3z1e3vJXef3wLwufT7jQBuT79/a3r/UwGsSp9nqMZ2XZ77G/po1q5u72lN7foggL8qeOwZAJ5Jv65Iv19RV7s67v87AD5fw/n6OQCXAHjUcPtVAL4GQABcBuAhm+eqkR61qh5Q1V67yb4dwEFVfUZV/w/AlwFcIyICYB2AXen9bgWwwVLTrkmfr+zzXgvga6p61NLrm1Rt1+uaPl+q+qSqPpV+/wKAFwEUzr4aUOHfS5f27gJwRXp+rgHwZVV9VVWfBXAwfb5a2qWq9+f+hh4EcK6l1x6oXV2sB3Cvqv5QVV8GcC+A9zTUrg8AuM3Saxup6r8h6ZSZXAPgS5p4EMByETkbls6VzznqMQDfzf38fHpsJYAjqvpax3EbzlLVQ+n33wNwVo/7b8TJfyTb0o8+nxaRU2tu12kiMisiD2bpGHh0vkTk7Uh6SU/nDts6X6a/l8L7pOfjFSTnp8xjXbYr78NIemaZove0znb9Yvr+7BKR8yo+1mW7kKaIVgHYkzvs6nz1Ymq3lXPlbIcXEbkPwI8X3LRVVf/R1ev20q1d+R9UVUXEWLuYXi0vAnBP7vANSALWKUjqKf8AwB/X2K4JVZ0XkfMB7BGR/UiCUd8sn6+/A3Cdqp5ID/d9vmIkIpsBTAF4Z+7wSe+pqj5d/AzW/ROA21T1VRH5TSSfRtbV9NplbASwS1WP5441eb6ccRaoVfXKAZ9iHsB5uZ/PTY+9hORjxdK0V5QdH7hdIvJ9ETlbVQ+lgeXFLk/1ywC+qqrHcs+d9S5fFZEvAPhkne1S1fn06zMi8gCANQC+gobPl4i8BcBuJBfpB3PP3ff5KmD6eym6z/MishTA6Uj+nso81mW7ICJXIrn4vVNVX82OG95TG4GnZ7tU9aXcj7cgGZPIHvuujsc+YKFNpdqVsxHAb+cPODxfvZjabeVc+Zz6+E8AF0hSsXAKkjflLk0y9PcjyQ8DwHUAbPXQ70qfr8zznpQbS4NVlhfeAKBwhNhFu0RkRZY6EJEzAawF8HjT5yt9776KJH+3q+M2m+er8O+lS3uvBbAnPT93AdgoSVXIKgAXAPiPAdpSqV0isgbA3wC4WlVfzB0vfE9rbNfZuR+vBnAg/f4eAO9O27cCwLux+JOl03albbsQyeDcN3LHXJ6vXu4C8Gtp9cdlAF5JOyJ2zpWLEdJe/wD8ApJczasAvg/gnvT4OQDuzt3vKgBPIrkibs0dPx/Jf6SDAP4BwKmW2rUSwNcBPAXgPgBnpMenANySu98kkivlko7H7wGwH0nA2QHgTXW1C8A70tf+Vvr1wz6cLwCbARwD8Eju38UuzlfR3wuSVMrV6fenpb//wfR8nJ977Nb0cU8A+HnLf++92nVf+v8gOz939XpPa2rXjQAeS1//fgAX5h776+l5PAjgQ3W2K/35jwBs73ics/OFpFN2KP1bfh7JWMJHAHwkvV0AfCZt837kqtlsnCtOISci8pzPqQ8iIgIDNRGR9xioiYg8x0BNROQ5BmoiIs8xUBMReY6BmojIc/8P0Zyc8n4kUGoAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layers = [\n",
        "    Layer(10, 'sigmoid'), \n",
        "    Layer(50, 'sigmoid'), \n",
        "    Layer(135, 'sigmoid'),\n",
        "    OutputLayer(2, 'sigmoid')\n",
        "]"
      ],
      "metadata": {
        "id": "yoAD6bVeWg0y"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn = NeuralNetwork(layers, 2) #2 is number of featuers"
      ],
      "metadata": {
        "id": "UcdcPYGpWmqk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKD3Rvbi59Qh",
        "outputId": "f7d3f5cc-5f67-4ba5-dcda-a28e3f48d2e0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hold = nn.predict(X)"
      ],
      "metadata": {
        "id": "KI5bwv9PWnHA"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(hold.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZuNk4DPZV37",
        "outputId": "a071d3e6-bcdb-47da-f559-53d717a5d404"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.where(y, 1, 0)\n",
        "b = np.where(y, 0, 1)\n",
        "y_ = np.c_[a, b]"
      ],
      "metadata": {
        "id": "YIQ7q67DOczK"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn.train(X, y_, 1e-3, 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6uDIC7X00eZ",
        "outputId": "2692968b-7bdd-45ec-d249-b659f4883468"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration: 0    Loss = 0.4379867795622971\n",
            "iteration: 1    Loss = 0.37805826956800315\n",
            "iteration: 2    Loss = 0.32862228318599807\n",
            "iteration: 3    Loss = 0.2874479642820455\n",
            "iteration: 4    Loss = 0.25285763628106706\n",
            "iteration: 5    Loss = 0.2235715165782612\n",
            "iteration: 6    Loss = 0.19860070586556716\n",
            "iteration: 7    Loss = 0.1771721043484486\n",
            "iteration: 8    Loss = 0.15867486143286308\n",
            "iteration: 9    Loss = 0.14262161034563337\n",
            "iteration: 10    Loss = 0.1286200208198428\n",
            "iteration: 11    Loss = 0.11635166081327723\n",
            "iteration: 12    Loss = 0.10555610731737966\n",
            "iteration: 13    Loss = 0.09601887493932364\n",
            "iteration: 14    Loss = 0.08756215398087196\n",
            "iteration: 15    Loss = 0.08003763864874172\n",
            "iteration: 16    Loss = 0.07332092605173894\n",
            "iteration: 17    Loss = 0.06730710689093095\n",
            "iteration: 18    Loss = 0.06190726826197333\n",
            "iteration: 19    Loss = 0.057045700383950945\n",
            "iteration: 20    Loss = 0.05265765082537221\n",
            "iteration: 21    Loss = 0.04868750768524054\n",
            "iteration: 22    Loss = 0.045087321176801885\n",
            "iteration: 23    Loss = 0.04181559391925995\n",
            "iteration: 24    Loss = 0.03883628591239177\n",
            "iteration: 25    Loss = 0.03611799203251131\n",
            "iteration: 26    Loss = 0.033633258935880005\n",
            "iteration: 27    Loss = 0.031358015203464006\n",
            "iteration: 28    Loss = 0.029271093931803892\n",
            "iteration: 29    Loss = 0.027353831152319763\n",
            "iteration: 30    Loss = 0.025589726730239678\n",
            "iteration: 31    Loss = 0.02396415696681935\n",
            "iteration: 32    Loss = 0.022464130163665743\n",
            "iteration: 33    Loss = 0.021078078026757457\n",
            "iteration: 34    Loss = 0.0197956770812787\n",
            "iteration: 35    Loss = 0.01860769530731834\n",
            "iteration: 36    Loss = 0.017505860044398244\n",
            "iteration: 37    Loss = 0.016482743891739985\n",
            "iteration: 38    Loss = 0.015531665883467535\n",
            "iteration: 39    Loss = 0.014646605669121498\n",
            "iteration: 40    Loss = 0.013822128799715347\n",
            "iteration: 41    Loss = 0.013053321524061731\n",
            "iteration: 42    Loss = 0.01233573375147885\n",
            "iteration: 43    Loss = 0.011665329045408266\n",
            "iteration: 44    Loss = 0.011038440685726885\n",
            "iteration: 45    Loss = 0.010451732982057234\n",
            "iteration: 46    Loss = 0.009902167141357872\n",
            "iteration: 47    Loss = 0.009386971094511152\n",
            "iteration: 48    Loss = 0.008903612772069339\n",
            "iteration: 49    Loss = 0.008449776391417444\n",
            "iteration: 50    Loss = 0.008023341378599351\n",
            "iteration: 51    Loss = 0.00762236359979806\n",
            "iteration: 52    Loss = 0.007245058621517423\n",
            "iteration: 53    Loss = 0.0068897867560110395\n",
            "iteration: 54    Loss = 0.006555039680617436\n",
            "iteration: 55    Loss = 0.006239428447116875\n",
            "iteration: 56    Loss = 0.005941672720816864\n",
            "iteration: 57    Loss = 0.005660591109408298\n",
            "iteration: 58    Loss = 0.005395092459065676\n",
            "iteration: 59    Loss = 0.005144168010476236\n",
            "iteration: 60    Loss = 0.004906884320474676\n",
            "iteration: 61    Loss = 0.004682376866393731\n",
            "iteration: 62    Loss = 0.004469844260049669\n",
            "iteration: 63    Loss = 0.004268543006896037\n",
            "iteration: 64    Loss = 0.004077782753365569\n",
            "iteration: 65    Loss = 0.003896921971939809\n",
            "iteration: 66    Loss = 0.003725364039251557\n",
            "iteration: 67    Loss = 0.0035625536674876836\n",
            "iteration: 68    Loss = 0.003407973653824822\n",
            "iteration: 69    Loss = 0.003261141916441602\n",
            "iteration: 70    Loss = 0.003121608789116081\n",
            "iteration: 71    Loss = 0.002988954549383279\n",
            "iteration: 72    Loss = 0.0028627871579114434\n",
            "iteration: 73    Loss = 0.002742740189075093\n",
            "iteration: 74    Loss = 0.002628470934826099\n",
            "iteration: 75    Loss = 0.0025196586657553767\n",
            "iteration: 76    Loss = 0.0024160030349217104\n",
            "iteration: 77    Loss = 0.002317222611456014\n",
            "iteration: 78    Loss = 0.002223053532252544\n",
            "iteration: 79    Loss = 0.0021332482612141344\n",
            "iteration: 80    Loss = 0.0020475744465403745\n",
            "iteration: 81    Loss = 0.0019658138674990325\n",
            "iteration: 82    Loss = 0.00188776146289696\n",
            "iteration: 83    Loss = 0.0018132244342505398\n",
            "iteration: 84    Loss = 0.001742021417295028\n",
            "iteration: 85    Loss = 0.0016739817160583024\n",
            "iteration: 86    Loss = 0.0016089445942826573\n",
            "iteration: 87    Loss = 0.0015467586194338444\n",
            "iteration: 88    Loss = 0.0014872810549836563\n",
            "iteration: 89    Loss = 0.0014303772970331286\n",
            "iteration: 90    Loss = 0.0013759203516976886\n",
            "iteration: 91    Loss = 0.0013237903499931293\n",
            "iteration: 92    Loss = 0.0012738740972444052\n",
            "iteration: 93    Loss = 0.0012260646543039285\n",
            "iteration: 94    Loss = 0.0011802609480918828\n",
            "iteration: 95    Loss = 0.0011363674091886002\n",
            "iteration: 96    Loss = 0.0010942936344098947\n",
            "iteration: 97    Loss = 0.001053954072450225\n",
            "iteration: 98    Loss = 0.0010152677308587215\n",
            "iteration: 99    Loss = 0.0009781579027469164\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ADD Relu, leaky Relu, Softmax\n",
        "#load momentum from my own notebook\n",
        "#tune the hyperparamter(learning_rate, #layers, #neurons/layer) till acheive a good results\n",
        "#write the report"
      ],
      "metadata": {
        "id": "bVmaAsD5FWbm"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hJYSG5vERTd1"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}