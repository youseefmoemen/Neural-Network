{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "GON6iRSbG-PX"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cRDdKL0M3thJ",
    "outputId": "521f2f2c-5186-485a-83a8-71474ed5e162"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1, -2, -3],\n",
       "       [-1, -2, -3]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = np.array([[1, 2, 3], \n",
    "          [1, 2, 3]])\n",
    "-1*d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "tv84SdZlPrxd"
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "  return 1 / (1 + np.exp(-1 * (z % 709)))\n",
    "def sigmoid_grad(z):\n",
    "  return z * (1 - z)\n",
    "def leakyRelu(z):\n",
    "  return np.where(z >= 0, z, z * 0.3)\n",
    "def leakyRelu_grad(z):\n",
    "  return np.where(z >= 0, 1, 0.3)\n",
    "def softmax(z):\n",
    "  e = np.exp(z)\n",
    "  return e / np.sum(e)\n",
    "def softmax_grad(z):\n",
    "    I = np.eye(z.shape)\n",
    "    return np.dot((I - softmax(z)).T, softmax(z))\n",
    "def tanh(z):\n",
    "  return np.tanh(z)\n",
    "def tanh_grad(z):\n",
    "  return 1 - np.power(tanh(z), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "A8eh2OJgP7IB"
   },
   "outputs": [],
   "source": [
    "Activations = {\n",
    "    'sigmoid': sigmoid,\n",
    "    'leakyRelu': leakyRelu,\n",
    "    'softmax': softmax,\n",
    "    'tanh': tanh\n",
    "}\n",
    "Activations_grad = {\n",
    "    'sigmoid': sigmoid_grad,\n",
    "    'leakyRelu': leakyRelu_grad,\n",
    "    'softmax': softmax_grad,\n",
    "    'tanh': tanh_grad\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "15j6dXKLIWN_"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "1Vx8P1VUHdk0"
   },
   "outputs": [],
   "source": [
    "mnist = fetch_openml('mnist_784', version = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "KVLMhO_MIYzq"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "mXY0wY3DIbFX"
   },
   "outputs": [],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits = 1, train_size=10000, test_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "D1VbSFuaIcp5"
   },
   "outputs": [],
   "source": [
    "for train_index, test_index in sss.split(mnist.data, mnist.target):\n",
    "  X_train = mnist.data.iloc[train_index] \n",
    "  y_train = mnist.target[train_index]\n",
    "  X_test = mnist.data.iloc[test_index] \n",
    "  y_test = mnist.target[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "CKFS19L0Ierk"
   },
   "outputs": [],
   "source": [
    "def clip_image(img, clipping_size = 0):\n",
    "  img = img.reshape((28, 28))\n",
    "  img = np.delete(img, range(clipping_size), 0)\n",
    "  img = np.delete(img, range(clipping_size), 1)\n",
    "  return img.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "75II8JDuIhF3"
   },
   "outputs": [],
   "source": [
    "def momentum(img, step_size, n_steps, clipping_size):\n",
    "  img = img.reshape((28 - clipping_size, 28-clipping_size))\n",
    "  momentums = []\n",
    "  for i in range(n_steps):\n",
    "    x = np.array(list(range(i*step_size, (i+1)*step_size)))\n",
    "    for j in range(n_steps):\n",
    "      pixels = img[j*step_size: (j+1)*step_size, i*step_size: (i+1) * step_size]\n",
    "      y =  np.array(list(range(j*step_size, (j+1)*step_size))).reshape(step_size,1)\n",
    "      area = np.sum(np.sum(pixels))\n",
    "      x_c = np.sum(np.sum(x.T * pixels)) / (area + 1e-10)\n",
    "      y_c = np.sum(np.sum(y * pixels)) / (area + 1e-10)\n",
    "      momentums.append((x_c, y_c))\n",
    "  return momentums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "_hZcCsCXIimx"
   },
   "outputs": [],
   "source": [
    "def reduction(data = None, n_momentums = 9):\n",
    "  n_steps = math.floor(np.sqrt(n_momentums))\n",
    "  clipping_size = 28 % n_steps\n",
    "  step_size = (28 - clipping_size) // n_steps\n",
    "  data = np.apply_along_axis(clipping_size = clipping_size\n",
    "                      , func1d = clip_image, axis = 1, arr = data)\n",
    "  momentums = np.apply_along_axis(step_size = step_size, n_steps = n_steps,\n",
    "                                  clipping_size= clipping_size,\n",
    "                                  func1d = momentum, axis = 1, arr = data)\n",
    "  return data, momentums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "XeGU_ZnMIkCS"
   },
   "outputs": [],
   "source": [
    "data, momentums = reduction(np.array(X_train.copy()), n_momentums = 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "LPxc-SxvKTXV"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lbz = LabelBinarizer()\n",
    "y_train = lbz.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "qKpQupWzGW3F"
   },
   "outputs": [],
   "source": [
    "X_train = momentums.reshape((10000, 18))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GZsZqWp-j6ah",
    "outputId": "b54486c1-d5af-4fec-a172-5bc98590bb0a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qQ0w9bcTr1tH",
    "outputId": "7b7a5a45-4bc8-45f0-fc55-5bf623df4b6b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 18)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0:0+1,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "id": "6F4u8TNXlgs1"
   },
   "outputs": [],
   "source": [
    "class weights:\n",
    "    def __init__(self, pre_layer, post_layer):\n",
    "        self.pre_layer = pre_layer\n",
    "        self.post_layer = post_layer\n",
    "        self.weight_init = np.random.normal(size=(self.pre_layer,self.post_layer))\n",
    "\n",
    "    def update(self, learning_rate, Dw):\n",
    "        self.weight_init  = self.weight_init +  learning_rate * np.transpose(Dw)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "id": "-v5Lkksr4xb5"
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    def __init__(self, layers, input_shape, nodes):\n",
    "      self.layers = layers\n",
    "      self.input_shape = input_shape\n",
    "      self.nodes = nodes\n",
    "      self.results = np.zeros(shape=(self.layers,),dtype=weights)\n",
    "      self.weight = np.zeros(shape=(self.layers - 1,),dtype=weights)\n",
    "\n",
    "    def initializing(self,train):\n",
    "        data = train.copy()\n",
    "        self.weight[0] = weights(data.shape[1], self.nodes)\n",
    "        self.weight[self.layers - 2] = weights(self.nodes, 10)\n",
    "\n",
    "\n",
    "        self.results[0] = weights(1, data.shape[1])\n",
    "        self.results[self.layers - 1] = weights(1, 10)\n",
    "\n",
    "        for i in range(1,self.layers - 2):\n",
    "            self.weight[i] = weights(self.nodes, self.nodes)\n",
    "\n",
    "        for i in range(1,self.layers - 1):\n",
    "            self.results[i] = weights(1, self.nodes)\n",
    "\n",
    "\n",
    "        return self.weight\n",
    "  \n",
    "    def forward_prop(self, train, weights):\n",
    "        d_set = train.copy()\n",
    "\n",
    "         \n",
    "        #self.results[0] = weights(d_set.shape[0], d_set.shape[1])\n",
    "        \n",
    "        self.results[0].weight_init = d_set.copy()\n",
    "\n",
    "        for i in range(1,self.layers-1):\n",
    "\n",
    "            d_set = sigmoid(np.dot(d_set, weights[i-1].weight_init))\n",
    "\n",
    "            \n",
    "            #self.results[i] = weights(d_set.shape[0], d_set.shape[1])\n",
    "            self.results[i].weight_init = d_set.copy()\n",
    "          \n",
    "        #self.results[self.layers - 1] = weights(d_set.shape[0], d_set.shape[1])\n",
    "        self.results[self.layers - 1].weights_init = d_set\n",
    "\n",
    "        return weights, self.results\n",
    "\n",
    "\n",
    "    \n",
    "    def back_propagation(self, results,y_train, weights, learning_rate):\n",
    "        ground_truth = y_train.copy()\n",
    "        weights_after = weights.copy()\n",
    "        weights_before = weights.copy()\n",
    "        output = results.copy()\n",
    "\n",
    "        s_delta = np.multiply((ground_truth - output[-1].weight_init), sigmoid_grad(output[-1].weight_init))\n",
    "\n",
    "        weights_after[-1].update(learning_rate, np.dot(np.transpose(s_delta), output[-2].weight_init))\n",
    "\n",
    "\n",
    "\n",
    "        for i in range(output.shape[0] - 2, 0, -1):\n",
    "            s_delta = np.dot(s_delta,np.transpose(weights_before[i].weight_init))\n",
    "            s_delta = np.multiply(s_delta, sigmoid_grad(output[i].weight_init))\n",
    "\n",
    "            weights_after[i-1].update(learning_rate, np.dot(np.transpose(s_delta), output[i-1].weight_init))\n",
    "\n",
    "        return weights_after\n",
    "\n",
    "  \n",
    "    def train(self, X_train, y_train, learning_rate, num_iterations, set_weight):\n",
    "      weights = set_weight.copy()\n",
    "      data = X_train.copy()\n",
    "      \n",
    "      for epochs in range(num_iterations):\n",
    "          for i in range(0,X_train.shape[0]):\n",
    "              weights, outputs = self.forward_prop(data[i:i+1,:],weights)\n",
    "              weights = self.back_propagation(outputs,y_train[i],weights, learning_rate)\n",
    "\n",
    "      return weights         \n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, X, Y, weights):\n",
    "        x = X.copy()\n",
    "\n",
    "        for i in range(weights.shape[0]):\n",
    "            x = sigmoid(np.dot(x,weights[i].weight_init))\n",
    "\n",
    "        wide_train = np.zeros([Y.shape[0],Y.shape[1]])\n",
    "\n",
    "\n",
    "        for m in range(0,Y.shape[0]):\n",
    "            index_ = 0\n",
    "            biggest = x[m,index_]\n",
    "    \n",
    "            for n in range(1,Y.shape[1]):\n",
    "                if biggest < x[m,n]:\n",
    "                    biggest = x[m,n]\n",
    "                    index_ = n\n",
    "            \n",
    "            wide_train[m,index_] = 1    \n",
    "\n",
    "\n",
    "        cnt = 0\n",
    "\n",
    "        for r in range(x.shape[0]):\n",
    "            for s in range(x.shape[1]):\n",
    "                if Y[r,s] == 1 and wide_train[r,s] == 1:\n",
    "                    cnt = cnt + 1\n",
    "\n",
    "        print((cnt/Y.shape[0])*100)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "id": "9N3aXaVojvO9"
   },
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(4,X_train,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "id": "dyOY_w5dUTdH"
   },
   "outputs": [],
   "source": [
    "W = nn.initializing(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TPURFbyHS4Wn",
    "outputId": "2f6d6299-e316-4656-dbf6-4018c4c1487b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 18)"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "id": "M08are5qRAIF"
   },
   "outputs": [],
   "source": [
    "W = nn.train(X_train,y_train,0.5,100,W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LP4Cl6gYU3Ua",
    "outputId": "e4222251-e673-4ffe-add5-cd5075327f2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.87\n"
     ]
    }
   ],
   "source": [
    "nn.predict(X_train,y_train,W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RHgXEul-T8IZ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "yousef_osama.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
